(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{370:function(t,a,s){"use strict";s.r(a);var n=s(42),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"cross-validation-intermediate"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#cross-validation-intermediate"}},[t._v("#")]),t._v(" Cross Validation (Intermediate)")]),t._v(" "),s("blockquote",[s("p",[t._v("How to perform various Cross Validation methodologies")])]),t._v(" "),s("hr"),t._v(" "),s("p",[t._v("This article is also a Jupyter Notebook available to be run from the top down. There\nwill be code snippets that you can then run in any environment.")]),t._v(" "),s("p",[t._v("Below are the versions of "),s("code",[t._v("fastai")]),t._v(", "),s("code",[t._v("fastcore")]),t._v(", "),s("code",[t._v("scikit-learn")]),t._v(", and "),s("code",[t._v("iterative-stratification")]),t._v(" currently running at the time of writing this:")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("fastai")]),t._v(": 2.0.14")]),t._v(" "),s("li",[s("code",[t._v("fastcore")]),t._v(": 1.0.14")]),t._v(" "),s("li",[s("code",[t._v("scikit-learn")]),t._v(": 0.22.2.post1")]),t._v(" "),s("li",[s("code",[t._v("iterative-stratification")]),t._v(": 0.1.6")])]),t._v(" "),s("hr"),t._v(" "),s("h1",{attrs:{id:"introduction"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),s("p",[t._v("In this tutorial we will show how to use various cross validation methodologies inside of "),s("code",[t._v("fastai")]),t._v("  with the "),s("code",[t._v("tabular")]),t._v(" and "),s("code",[t._v("vision")]),t._v(" libraries. First, let's walk through a "),s("code",[t._v("tabular")]),t._v(" example")]),t._v(" "),s("h1",{attrs:{id:"tabular"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tabular"}},[t._v("#")]),t._v(" Tabular")]),t._v(" "),s("h2",{attrs:{id:"importing-the-library-and-the-dataset"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#importing-the-library-and-the-dataset"}},[t._v("#")]),t._v(" Importing the Library and the Dataset")]),t._v(" "),s("p",[t._v("We'll be using the "),s("code",[t._v("tabular")]),t._v(" module for the first example, along with the "),s("code",[t._v("ADULTS")]),t._v(" dataset. Let's grab those:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fastai"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tabular"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("all")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ADULT_SAMPLE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Let's open it in "),s("code",[t._v("Pandas")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("df "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'adult.csv'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("age")]),t._v(" "),s("th",[t._v("workclass")]),t._v(" "),s("th",[t._v("fnlwgt")]),t._v(" "),s("th",[t._v("education")]),t._v(" "),s("th",[t._v("education-num")]),t._v(" "),s("th",[t._v("marital-status")]),t._v(" "),s("th",[t._v("occupation")]),t._v(" "),s("th",[t._v("relationship")]),t._v(" "),s("th",[t._v("race")]),t._v(" "),s("th",[t._v("sex")]),t._v(" "),s("th",[t._v("capital-gain")]),t._v(" "),s("th",[t._v("capital-loss")]),t._v(" "),s("th",[t._v("hours-per-week")]),t._v(" "),s("th",[t._v("native-country")]),t._v(" "),s("th",[t._v("salary")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("49")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("101320")]),t._v(" "),s("td",[t._v("Assoc-acdm")]),t._v(" "),s("td",[t._v("12.0")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("1902")]),t._v(" "),s("td",[t._v("40")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("44")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("236746")]),t._v(" "),s("td",[t._v("Masters")]),t._v(" "),s("td",[t._v("14.0")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("Exec-managerial")]),t._v(" "),s("td",[t._v("Not-in-family")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("Male")]),t._v(" "),s("td",[t._v("10520")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("45")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("38")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("96185")]),t._v(" "),s("td",[t._v("HS-grad")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Unmarried")]),t._v(" "),s("td",[t._v("Black")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("32")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("38")]),t._v(" "),s("td",[t._v("Self-emp-inc")]),t._v(" "),s("td",[t._v("112847")]),t._v(" "),s("td",[t._v("Prof-school")]),t._v(" "),s("td",[t._v("15.0")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Prof-specialty")]),t._v(" "),s("td",[t._v("Husband")]),t._v(" "),s("td",[t._v("Asian-Pac-Islander")]),t._v(" "),s("td",[t._v("Male")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("40")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("42")]),t._v(" "),s("td",[t._v("Self-emp-not-inc")]),t._v(" "),s("td",[t._v("82297")]),t._v(" "),s("td",[t._v("7th-8th")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Other-service")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("Black")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("50")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v("<50k")])])])])]),t._v(" "),s("p",[t._v("Next we want to create a constant test set and declare our various variables and "),s("code",[t._v("procs")]),t._v(". We'll just be using the last 10% of the data, however figuring out how to make your test set is a very important problem. To read more, see Rachel Thomas' article on "),s("a",{attrs:{href:"https://www.fast.ai/2017/11/13/validation-sets/",target:"_blank",rel:"noopener noreferrer"}},[t._v("How (and why) to create a good validation set"),s("OutboundLink")],1),t._v(".\n{% include note.html content='we call it a test set here as we make our own mini validation sets when weâ€™re training' %}")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("cat_names "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'workclass'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'marital-status'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'occupation'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relationship'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'race'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ncont_names "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'age'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fnlwgt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education-num'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nprocs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Categorify"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FillMissing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("p",[t._v("And now we'll split our dataset:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'10% of our data is ")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v(".1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v(" rows'")])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("10% of our data is 3256 rows\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("start_val "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3256")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" start_val\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("29305\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("train "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("start_val"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ntest "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("start_val"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("p",[t._v("Now that we have the "),s("code",[t._v("DataFrames")]),t._v(", let's look into a few different CV methods:")]),t._v(" "),s("h2",{attrs:{id:"k-fold"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#k-fold"}},[t._v("#")]),t._v(" K-Fold")]),t._v(" "),s("p",[t._v("Every Cross Validation method is slightly different, and what version you should use depends on the dataset you are utilizing. The general idea of Cross Validation is we split the dataset into "),s("code",[t._v("n")]),t._v(" sets (usually five is enough), train five seperate models, and then at the end we can ensemble them together. This should in theory make a group of models that performs better than one model on the entire dataset.")]),t._v(" "),s("p",[t._v("As we are training, there is zero overlap in the validation sets whatsoever. As a result we create five distinct validation sets.")]),t._v(" "),s("h3",{attrs:{id:"introduction-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#introduction-2"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),s("p",[t._v("Now for the "),s("code",[t._v("kfold")]),t._v(". We'll first be using "),s("code",[t._v("sklearn")]),t._v("'s "),s("code",[t._v("KFold")]),t._v(" class. This method works by running through all the indicies available and seperating out the folds. For a minimum example, take the following:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("train_idxs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_idxs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("p",[t._v("We now have some training indicies and a test set:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("train_idxs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" test_idxs\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("([0, 1, 2, 3, 4, 5, 6, 7, 8], [10])\n")])])]),s("p",[t._v("Now we can instantiate a "),s("code",[t._v("KFold")]),t._v(" object, passing in the number of splits, whether to shuffle the data before splitting into folds, and potentially a seed:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_selection "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" KFold\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dummy_kf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" KFold"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_splits"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shuffle"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" dummy_kf\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("KFold(n_splits=5, random_state=None, shuffle=False)\n")])])]),s("p",[t._v("And now we can run through our splits by iterating through train and valid indexes. We pass in our "),s("code",[t._v("x")]),t._v(" data through "),s("code",[t._v("dummy_kf.split")]),t._v(" to get the indexes")]),t._v(" "),s("blockquote",[s("p",[t._v("You could also pass in your "),s("code",[t._v("y")]),t._v("'s intead:")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" train_idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_idx "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" dummy_kf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_idxs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'Train: ")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("train_idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v(", Valid: ")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("valid_idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("Train: [2 3 4 5 6 7 8], Valid: [0 1]\nTrain: [0 1 4 5 6 7 8], Valid: [2 3]\nTrain: [0 1 2 3 6 7 8], Valid: [4 5]\nTrain: [0 1 2 3 4 5 8], Valid: [6 7]\nTrain: [0 1 2 3 4 5 6 7], Valid: [8]\n")])])]),s("h3",{attrs:{id:"extra-preprocessing"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#extra-preprocessing"}},[t._v("#")]),t._v(" Extra Preprocessing")]),t._v(" "),s("p",[t._v("Now the question is how can I use this when training on our data?")]),t._v(" "),s("p",[t._v("When we preprocess our tabular training dataset, we build our "),s("code",[t._v("procs")]),t._v(" based upon it. When doing a CV (Cross Validation) we will often exclude some data as it gets pushed to the validation set, leading to such errors as:")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("---------------------------------------------------------------------------\n\nAssertionError                            Traceback (most recent call last)\n\n<ipython-input-16-59d14331bcbc> in <module>()\n      1 #hide_input\n----\x3e 2 raise AssertionError('nan values in `education-num` but not in setup training set')\n\n\nAssertionError: nan values in `education-num` but not in setup training set\n")])])]),s("p",[t._v("So how do we fix this? We should preprocess the entire training "),s("code",[t._v("DataFrame")]),t._v(" into "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" first, this way we can extract all the "),s("code",[t._v("proc")]),t._v(" information. Let's do that now:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to_base "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularPandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" procs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cont_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'salary'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Next we need to extract all the information we need. This includes:")]),t._v(" "),s("ul",[s("li",[s("RouterLink",{attrs:{to:"/tab.stats.html#Categorify"}},[s("code",[t._v("Categorify")])]),t._v("'s classes")],1),t._v(" "),s("li",[s("a",{attrs:{href:"https://docs.fast.ai/data.transforms#Normalize",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Normalize")]),s("OutboundLink")],1),t._v("'s "),s("code",[t._v("means")]),t._v(" and "),s("code",[t._v("stds")])]),t._v(" "),s("li",[s("RouterLink",{attrs:{to:"/tab.stats.html#FillMissing"}},[s("code",[t._v("FillMissing")])]),t._v("'s "),s("code",[t._v("fill_vals")]),t._v(" and "),s("code",[t._v("na_dict")])],1)]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("classes "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to_base"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("classes\nmeans"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" stds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to_base"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("means"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_base"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stds\nfill_vals"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" na_dict "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to_base"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fill_missing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fill_vals"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_base"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fill_missing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("na_dict\n")])])]),s("p",[t._v("Now we could generate new procs based on those and apply them to our dataset:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("procs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Categorify"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("classes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_tab"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("means"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" stds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FillMissing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fill_strategy"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("FillStrategy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("median"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fill_vals"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("fill_vals"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" na_dict"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("na_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("h3",{attrs:{id:"now-let-s-train"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#now-let-s-train"}},[t._v("#")]),t._v(" Now Let's Train")]),t._v(" "),s("p",[t._v("Now that we have our adjusted "),s("code",[t._v("procs")]),t._v(", let's try training.")]),t._v(" "),s("p",[t._v("We'll want to make a loop that will do the following:")]),t._v(" "),s("ol",[s("li",[t._v("Make our "),s("code",[t._v("KFold")]),t._v(" and split")]),t._v(" "),s("li",[t._v("Build a "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" object given our splits")]),t._v(" "),s("li",[t._v("Train for some training regiment")]),t._v(" "),s("li",[t._v("Get predictions on the "),s("a",{attrs:{href:"https://fastcore.fast.ai/test#test",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("test")]),s("OutboundLink")],1),t._v(" set, and potentially keep track of any statistics.")])]),t._v(" "),s("p",[t._v("Let's do so below:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("val_pct"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tst_preds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" L"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" L"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nkf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" KFold"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_splits"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shuffle"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" train_idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_idx "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" kf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    splits "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("L"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" L"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("valid_idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    procs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Categorify"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("classes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_tab"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("means"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" stds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FillMissing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fill_strategy"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("FillStrategy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("median"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fill_vals"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("fill_vals"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" na_dict"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("na_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    to "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularPandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" procs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cont_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'salary'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                       splits"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("splits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tabular_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" layers"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    test_dl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test_dl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("no_bar"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        val_pct"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("validate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        tst_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dl"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("test_dl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.379017")]),t._v(" "),s("td",[t._v("0.380708")]),t._v(" "),s("td",[t._v("0.826992")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.364980")]),t._v(" "),s("td",[t._v("0.359392")]),t._v(" "),s("td",[t._v("0.832281")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.355631")]),t._v(" "),s("td",[t._v("0.361775")]),t._v(" "),s("td",[t._v("0.825456")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.382340")]),t._v(" "),s("td",[t._v("0.376627")]),t._v(" "),s("td",[t._v("0.829039")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.362212")]),t._v(" "),s("td",[t._v("0.366542")]),t._v(" "),s("td",[t._v("0.832111")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.355434")]),t._v(" "),s("td",[t._v("0.372222")]),t._v(" "),s("td",[t._v("0.830063")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.385911")]),t._v(" "),s("td",[t._v("0.374170")]),t._v(" "),s("td",[t._v("0.843542")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.368800")]),t._v(" "),s("td",[t._v("0.339751")]),t._v(" "),s("td",[t._v("0.842348")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.360772")]),t._v(" "),s("td",[t._v("0.349895")]),t._v(" "),s("td",[t._v("0.843542")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.377877")]),t._v(" "),s("td",[t._v("0.358854")]),t._v(" "),s("td",[t._v("0.835523")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.362264")]),t._v(" "),s("td",[t._v("0.362680")]),t._v(" "),s("td",[t._v("0.833646")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.355874")]),t._v(" "),s("td",[t._v("0.363413")]),t._v(" "),s("td",[t._v("0.833134")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.380469")]),t._v(" "),s("td",[t._v("0.358595")]),t._v(" "),s("td",[t._v("0.838423")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.363201")]),t._v(" "),s("td",[t._v("0.352324")]),t._v(" "),s("td",[t._v("0.837912")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.356388")]),t._v(" "),s("td",[t._v("0.350427")]),t._v(" "),s("td",[t._v("0.837741")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("p",[t._v("Now let's take a look at our results:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" truth"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tst_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'Fold ")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v(": ")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" truth"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("Fold 1: 0.8390663266181946\nFold 2: 0.834152340888977\nFold 3: 0.8320024609565735\nFold 4: 0.8356879353523254\nFold 5: 0.8329238295555115\n")])])]),s("p",[t._v("Let's try ensembling them and seeing what happens:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("sum_preds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" truth"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tst_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    sum_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("numpy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\navg_preds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sum_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'Average Accuracy: ")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("avg_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tst_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("Average Accuracy: 0.8366093635559082\n")])])]),s("p",[t._v("As we can see, ensembling all the models together boosted our score by .1%. Not the highest of increases though! Let's try out another CV method and see if it works better")]),t._v(" "),s("h2",{attrs:{id:"stratified-k-fold"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#stratified-k-fold"}},[t._v("#")]),t._v(" Stratified K-Fold")]),t._v(" "),s("p",[t._v("While the first example simply split our dataset either randomly (if we passed "),s("code",[t._v("True")]),t._v(") or just down the indicies, there are a multitude of cases where we won't have perfectly balanced classes (where the previous example would be useful). What can we do in such a situation?")]),t._v(" "),s("p",[t._v("Stratified K-Fold Validation allows us to split our data while also preserving the percentage of samples inside of each class. We'll follow the same methodology as we did before with a few minor changes to have it work with Stratified K-Fold")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_selection "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" StratifiedKFold\n")])])]),s("p",[t._v("The only difference is along with our "),s("code",[t._v("train.index")]),t._v(" we also need to pass in our "),s("code",[t._v("y")]),t._v("'s so it can gather the class distributions:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("val_pct"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tst_preds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" L"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" L"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nskf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" StratifiedKFold"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_splits"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shuffle"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" train_idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_idx "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" kf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'salary'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# right here")]),t._v("\n    splits "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("L"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" L"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("valid_idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    procs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Categorify"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("classes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_tab"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("means"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" stds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FillMissing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fill_strategy"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("FillStrategy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("median"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fill_vals"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("fill_vals"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" na_dict"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("na_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    to "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularPandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" procs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cont_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'salary'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                       splits"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("splits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tabular_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" layers"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    test_dl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test_dl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("no_bar"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        val_pct"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("validate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        tst_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dl"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("test_dl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.377596")]),t._v(" "),s("td",[t._v("0.366456")]),t._v(" "),s("td",[t._v("0.831599")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.360850")]),t._v(" "),s("td",[t._v("0.361772")]),t._v(" "),s("td",[t._v("0.827674")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.356481")]),t._v(" "),s("td",[t._v("0.359992")]),t._v(" "),s("td",[t._v("0.831257")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.377417")]),t._v(" "),s("td",[t._v("0.388749")]),t._v(" "),s("td",[t._v("0.822726")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.360371")]),t._v(" "),s("td",[t._v("0.376890")]),t._v(" "),s("td",[t._v("0.824774")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.352614")]),t._v(" "),s("td",[t._v("0.368503")]),t._v(" "),s("td",[t._v("0.833817")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.387596")]),t._v(" "),s("td",[t._v("0.358673")]),t._v(" "),s("td",[t._v("0.842177")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.368236")]),t._v(" "),s("td",[t._v("0.347018")]),t._v(" "),s("td",[t._v("0.844907")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.362123")]),t._v(" "),s("td",[t._v("0.345612")]),t._v(" "),s("td",[t._v("0.841324")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.375481")]),t._v(" "),s("td",[t._v("0.365665")]),t._v(" "),s("td",[t._v("0.836205")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.358180")]),t._v(" "),s("td",[t._v("0.362090")]),t._v(" "),s("td",[t._v("0.832111")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.351982")]),t._v(" "),s("td",[t._v("0.360600")]),t._v(" "),s("td",[t._v("0.830404")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.385218")]),t._v(" "),s("td",[t._v("0.363116")]),t._v(" "),s("td",[t._v("0.831428")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.363915")]),t._v(" "),s("td",[t._v("0.349798")]),t._v(" "),s("td",[t._v("0.836717")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.356412")]),t._v(" "),s("td",[t._v("0.354061")]),t._v(" "),s("td",[t._v("0.837400")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("p",[t._v("Let's see how our new version fairs up:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" truth"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tst_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'Fold ")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v(": ")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" truth"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("Fold 1: 0.8335380554199219\nFold 2: 0.835073709487915\nFold 3: 0.8316953182220459\nFold 4: 0.8412162065505981\nFold 5: 0.8387592434883118\n")])])]),s("p",[t._v("We can see that so far it looks a bit better (we actually have one with 84%!).")]),t._v(" "),s("p",[t._v("Now let's try the ensemble:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("sum_preds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" truth"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tst_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    sum_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("numpy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\navg_preds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sum_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'Average Accuracy: ")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("avg_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tst_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("Average Accuracy: 0.835995078086853\n")])])]),s("p",[t._v("Not quite as well in the ensemble (down by 0.1%), however I would trust this version much "),s("em",[t._v("much")]),t._v(" more than the regular "),s("code",[t._v("KFold")]),t._v(".")]),t._v(" "),s("p",[t._v("Why?")]),t._v(" "),s("p",[t._v("Stratification ensures that we maintain the original distribution of our "),s("code",[t._v("y")]),t._v(" values, ensuring that if we have rare classes they will always show up and be trained on. Now let's look at a multi-label example.")]),t._v(" "),s("h2",{attrs:{id:"multi-label-stratified-k-fold"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#multi-label-stratified-k-fold"}},[t._v("#")]),t._v(" Multi-Label Stratified K-Fold")]),t._v(" "),s("p",[t._v("To run Multi-Label Stratified K-Fold, I will show an example below, but we will not run it (as there currently isn't quite a close enough dataset outside of Kaggle right now).")]),t._v(" "),s("p",[t._v("First we'll need to import our "),s("code",[t._v("MultilabelStratifiedKfold")]),t._v(" from "),s("code",[t._v("iterstrat")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" iterstrat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ml_stratifiers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" MultilabelStratifiedKFold\n")])])]),s("p",[t._v("Then when following our above method (ensure you have your "),s("code",[t._v("loss_func")]),t._v(", etc properly setup), we simply replace our "),s("code",[t._v("for train_idx, valid_idx")]),t._v(" with:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("mskf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" MultilabelStratifiedKFold"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_splits"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" train_idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val_idx "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" mskf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("y_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"blah"')]),t._v("\n")])])])])}),[],!1,null,null,null);a.default=e.exports}}]);