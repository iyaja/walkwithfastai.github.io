(window.webpackJsonp=window.webpackJsonp||[]).push([[12],{367:function(t,a,s){"use strict";s.r(a);var e=s(42),n=Object(e.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"exporting-tabularpandas-for-inference-intermediate"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#exporting-tabularpandas-for-inference-intermediate"}},[t._v("#")]),t._v(" Exporting "),s("code",[t._v("TabularPandas")]),t._v(" for Inference (Intermediate)")]),t._v(" "),s("blockquote",[s("p",[t._v("A guide for exporting "),s("code",[t._v("TabularPandas")]),t._v(" and use it for inference with non-neural networks")])]),t._v(" "),s("hr"),t._v(" "),s("p",[t._v("This article is also a Jupyter Notebook available to be run from the top down. There\nwill be code snippets that you can then run in any environment.")]),t._v(" "),s("p",[t._v("Below are the versions of "),s("code",[t._v("fastai")]),t._v(", "),s("code",[t._v("fastcore")]),t._v(", and "),s("code",[t._v("wwf")]),t._v(" currently running at the time of writing this:")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("fastai")]),t._v(": 2.0.16")]),t._v(" "),s("li",[s("code",[t._v("fastcore")]),t._v(": 1.1.2")]),t._v(" "),s("li",[s("code",[t._v("wwf")]),t._v(": 0.0.4")])]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"using-tabularpandas-as-a-preprocessor"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#using-tabularpandas-as-a-preprocessor"}},[t._v("#")]),t._v(" Using "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" as a Preprocessor")]),t._v(" "),s("p",[t._v("As mentioned in the "),s("a",{attrs:{href:"https://docs.fast.ai/tutorial.tabular#fastai-with-Other-Libraries",target:"_blank",rel:"noopener noreferrer"}},[t._v("documentation"),s("OutboundLink")],1),t._v(" using "),s("code",[t._v("fastai")]),t._v(" to preprocess our tabular data can be a nice way in which the library integrates with XGBoost and Random Forests.")]),t._v(" "),s("p",[t._v("The issue though is when doing inference currently there is no way to export our "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" object so we can do inference without building "),s("a",{attrs:{href:"https://docs.fast.ai/data.core#DataLoaders",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("DataLoaders")]),s("OutboundLink")],1),t._v(" and exporting a "),s("a",{attrs:{href:"https://docs.fast.ai/learner#Learner",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Learner")]),s("OutboundLink")],1),t._v(". We'll solve this problem here and explain what we are doing.")]),t._v(" "),s("p",[t._v("This is a much shorter article as it's currently an active "),s("a",{attrs:{href:"https://github.com/fastai/fastai/pull/2857",target:"_blank",rel:"noopener noreferrer"}},[t._v("PR"),s("OutboundLink")],1),t._v(", but it will live here until the functionality is merged.")]),t._v(" "),s("h2",{attrs:{id:"grab-the-data"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#grab-the-data"}},[t._v("#")]),t._v(" Grab the Data")]),t._v(" "),s("p",[t._v("Let's grab the "),s("code",[t._v("ADULT_SAMPLE")]),t._v(" dataset quickly and work with it:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fastai"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tabular"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("all")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ADULT_SAMPLE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'adult.csv'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("age")]),t._v(" "),s("th",[t._v("workclass")]),t._v(" "),s("th",[t._v("fnlwgt")]),t._v(" "),s("th",[t._v("education")]),t._v(" "),s("th",[t._v("education-num")]),t._v(" "),s("th",[t._v("marital-status")]),t._v(" "),s("th",[t._v("occupation")]),t._v(" "),s("th",[t._v("relationship")]),t._v(" "),s("th",[t._v("race")]),t._v(" "),s("th",[t._v("sex")]),t._v(" "),s("th",[t._v("capital-gain")]),t._v(" "),s("th",[t._v("capital-loss")]),t._v(" "),s("th",[t._v("hours-per-week")]),t._v(" "),s("th",[t._v("native-country")]),t._v(" "),s("th",[t._v("salary")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("49")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("101320")]),t._v(" "),s("td",[t._v("Assoc-acdm")]),t._v(" "),s("td",[t._v("12.0")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("1902")]),t._v(" "),s("td",[t._v("40")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("44")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("236746")]),t._v(" "),s("td",[t._v("Masters")]),t._v(" "),s("td",[t._v("14.0")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("Exec-managerial")]),t._v(" "),s("td",[t._v("Not-in-family")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("Male")]),t._v(" "),s("td",[t._v("10520")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("45")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("38")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("96185")]),t._v(" "),s("td",[t._v("HS-grad")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Unmarried")]),t._v(" "),s("td",[t._v("Black")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("32")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("38")]),t._v(" "),s("td",[t._v("Self-emp-inc")]),t._v(" "),s("td",[t._v("112847")]),t._v(" "),s("td",[t._v("Prof-school")]),t._v(" "),s("td",[t._v("15.0")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Prof-specialty")]),t._v(" "),s("td",[t._v("Husband")]),t._v(" "),s("td",[t._v("Asian-Pac-Islander")]),t._v(" "),s("td",[t._v("Male")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("40")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("42")]),t._v(" "),s("td",[t._v("Self-emp-not-inc")]),t._v(" "),s("td",[t._v("82297")]),t._v(" "),s("td",[t._v("7th-8th")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Other-service")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("Black")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("50")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v("<50k")])])])])]),t._v(" "),s("h2",{attrs:{id:"building-our-tabularpandas"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#building-our-tabularpandas"}},[t._v("#")]),t._v(" Building our "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("Next we'll want to make our sample "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" object. For our added "),s("code",[t._v("export")]),t._v(" and "),s("code",[t._v("import")]),t._v(" functionalities we will use the "),s("code",[t._v("@patch")]),t._v(" method out of "),s("code",[t._v("fastcore")]),t._v(" which means we can add them on later.")]),t._v(" "),s("p",[t._v("Let's build our "),s("code",[t._v("to")]),t._v(" object:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("cat_names "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'workclass'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'marital-status'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'occupation'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relationship'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'race'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ncont_names "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'age'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fnlwgt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education-num'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nprocs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("FillMissing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Categorify"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nsplits "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" RandomSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("range_of"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nto "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularPandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" procs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("procs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cat_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cont_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cont_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   y_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'salary'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" splits"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("splits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("The nice part about "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" is now our data is completely preprocessed, as we can see blow by looking at a few rows of our "),s("code",[t._v("xs")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("workclass")]),t._v(" "),s("th",[t._v("education")]),t._v(" "),s("th",[t._v("marital-status")]),t._v(" "),s("th",[t._v("occupation")]),t._v(" "),s("th",[t._v("relationship")]),t._v(" "),s("th",[t._v("race")]),t._v(" "),s("th",[t._v("education-num_na")]),t._v(" "),s("th",[t._v("age")]),t._v(" "),s("th",[t._v("fnlwgt")]),t._v(" "),s("th",[t._v("education-num")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("18966")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("10")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("11")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.030856")]),t._v(" "),s("td",[t._v("-0.858388")]),t._v(" "),s("td",[t._v("1.146335")])]),t._v(" "),s("tr",[s("th",[t._v("11592")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("13")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("-0.702314")]),t._v(" "),s("td",[t._v("-0.021613")]),t._v(" "),s("td",[t._v("1.537389")])]),t._v(" "),s("tr",[s("th",[t._v("548")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("16")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v("-0.115778")]),t._v(" "),s("td",[t._v("-0.336138")]),t._v(" "),s("td",[t._v("-0.026827")])]),t._v(" "),s("tr",[s("th",[t._v("32008")]),t._v(" "),s("td",[t._v("8")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.764027")]),t._v(" "),s("td",[t._v("0.129564")]),t._v(" "),s("td",[t._v("-1.199989")])]),t._v(" "),s("tr",[s("th",[t._v("23657")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("10")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("13")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.324124")]),t._v(" "),s("td",[t._v("1.550315")]),t._v(" "),s("td",[t._v("1.146335")])])])])]),t._v(" "),s("h2",{attrs:{id:"exporting-our-tabularpandas"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#exporting-our-tabularpandas"}},[t._v("#")]),t._v(" Exporting our "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("The next bit we want to do is actually add our export funcionality. We'll save it away as a pickle file:")]),t._v(" "),s("h4",{staticClass:"doc_header",attrs:{id:"TabularPandas.export"}},[s("code",[t._v("TabularPandas.export")]),s("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/walkwithfastai/walkwithfastai.github.io/tree/master/wwf/tab/export.py#L9"}},[t._v("[source]")])]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("TabularPandas.export")]),t._v("("),s("strong",[s("code",[t._v("fname")])]),t._v("="),s("em",[s("code",[t._v("'export.pkl'")])]),t._v(", "),s("strong",[s("code",[t._v("pickle_protocol")])]),t._v("="),s("em",[s("code",[t._v("2")])]),t._v(")")])]),t._v(" "),s("p",[t._v("Export the contents of "),s("code",[t._v("self")]),t._v(" without the items")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@patch")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("export")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("TabularPandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fname"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'export.pkl'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pickle_protocol"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Export the contents of `self` without the items"')]),t._v("\n    old_to "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self\n    self "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("new_empty"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" warnings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("catch_warnings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        warnings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("simplefilter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ignore"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        pickle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dump"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fname"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'wb'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" protocol"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pickle_protocol"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" old_to\n")])])]),s("p",[t._v("And now we can directly use it:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("export"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'to.pkl'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"loading-it-back-in"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#loading-it-back-in"}},[t._v("#")]),t._v(" Loading It Back In")]),t._v(" "),s("p",[t._v("Now that we have exported our "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(", how do we use it in deployment? We'll make a "),s("RouterLink",{attrs:{to:"/tab.export.html#load_pandas"}},[s("code",[t._v("load_pandas")])]),t._v(" function to bring our pickle in:")],1),t._v(" "),s("h4",{staticClass:"doc_header",attrs:{id:"load_pandas"}},[s("code",[t._v("load_pandas")]),s("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/walkwithfastai/walkwithfastai.github.io/tree/master/wwf/tab/export.py#L20"}},[t._v("[source]")])]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("load_pandas")]),t._v("("),s("strong",[s("code",[t._v("fname")])]),t._v(")")])]),t._v(" "),s("p",[t._v("Load in a "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" object from "),s("code",[t._v("fname")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("load_pandas")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fname"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Load in a `TabularPandas` object from `fname`"')]),t._v("\n    distrib_barrier"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    res "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pickle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fname"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rb'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" res\n")])])]),s("p",[t._v("Let's do so for our newly exported "),s("code",[t._v("to")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to_load "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" load_pandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'to.pkl'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("And we can see it has no data:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to_load"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("0\n")])])]),s("p",[t._v("So how do we process some new data? the key is a combination of two functions:")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("to.train.new()")])]),t._v(" "),s("li",[s("code",[t._v("to.process()")])])]),t._v(" "),s("p",[t._v("The first will setup our data as though it is based on our training data and the second will run our "),s("code",[t._v("procs")]),t._v(" through it. Let's try it out on a subset of our "),s("code",[t._v("DataFrame")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to_new "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to_load"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("new"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nto_new"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("And if we examine our data, we can see it's processed!")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to_new"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("workclass")]),t._v(" "),s("th",[t._v("education")]),t._v(" "),s("th",[t._v("marital-status")]),t._v(" "),s("th",[t._v("occupation")]),t._v(" "),s("th",[t._v("relationship")]),t._v(" "),s("th",[t._v("race")]),t._v(" "),s("th",[t._v("education-num_na")]),t._v(" "),s("th",[t._v("age")]),t._v(" "),s("th",[t._v("fnlwgt")]),t._v(" "),s("th",[t._v("education-num")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("8")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("6")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.764027")]),t._v(" "),s("td",[t._v("-0.840572")]),t._v(" "),s("td",[t._v("0.755281")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("13")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.397441")]),t._v(" "),s("td",[t._v("0.451042")]),t._v(" "),s("td",[t._v("1.537389")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("12")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v("-0.042461")]),t._v(" "),s("td",[t._v("-0.889547")]),t._v(" "),s("td",[t._v("-0.026827")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("6")]),t._v(" "),s("td",[t._v("15")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("11")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("-0.042461")]),t._v(" "),s("td",[t._v("-0.730635")]),t._v(" "),s("td",[t._v("1.928443")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("7")]),t._v(" "),s("td",[t._v("6")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("9")]),t._v(" "),s("td",[t._v("6")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.250807")]),t._v(" "),s("td",[t._v("-1.022003")]),t._v(" "),s("td",[t._v("-0.026827")])])])])]),t._v(" "),s("p",[t._v("To use this with your own projects simply make sure you've "),s("code",[t._v("pip")]),t._v(" installed "),s("code",[t._v("wwf")]),t._v(" and do:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" wwf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tabular"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("export "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n\nto"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("export"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fname"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("After training and do what we did above for using your exported "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" object with new data")])])}),[],!1,null,null,null);a.default=n.exports}}]);