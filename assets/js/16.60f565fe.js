(window.webpackJsonp=window.webpackJsonp||[]).push([[16],{361:function(t,a,s){"use strict";s.r(a);var n=s(42),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"utilizing-the-timm-library-inside-of-fastai-intermediate"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#utilizing-the-timm-library-inside-of-fastai-intermediate"}},[t._v("#")]),t._v(" Utilizing the "),s("code",[t._v("timm")]),t._v(" Library Inside of "),s("code",[t._v("fastai")]),t._v(" (Intermediate)")]),t._v(" "),s("blockquote",[s("p",[t._v("How to bring the power of Transfer Learning with new architectures")])]),t._v(" "),s("hr"),t._v(" "),s("p",[t._v("This article is also a Jupyter Notebook available to be run from the top down. There\nwill be code snippets that you can then run in any environment.")]),t._v(" "),s("p",[t._v("Below are the versions of "),s("code",[t._v("fastai")]),t._v(", "),s("code",[t._v("fastcore")]),t._v(", and "),s("code",[t._v("timm")]),t._v(" currently running at the time of writing this:")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("fastai")]),t._v(": 2.0.14")]),t._v(" "),s("li",[s("code",[t._v("fastcore")]),t._v(": 1.0.11")]),t._v(" "),s("li",[s("code",[t._v("timm")]),t._v(": 0.2.1")])]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"bringing-in-external-models-into-the-framework"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#bringing-in-external-models-into-the-framework"}},[t._v("#")]),t._v(" Bringing in External Models into the Framework")]),t._v(" "),s("p",[t._v("As we are well aware, "),s("code",[t._v("fastai")]),t._v(" models deep down are just "),s("code",[t._v("PyTorch")]),t._v(" models. However as the field of Machine Learning keeps going, new and fresh architectures are introduced. Wouldn't it be nice if it were easy to integrate them into the "),s("code",[t._v("fastai")]),t._v(" framework and play with them?")]),t._v(" "),s("h2",{attrs:{id:"using-ross-wightman-s-timm-library"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#using-ross-wightman-s-timm-library"}},[t._v("#")]),t._v(" Using Ross Wightman's "),s("code",[t._v("timm")]),t._v(" Library")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://twitter.com/wightmanr",target:"_blank",rel:"noopener noreferrer"}},[t._v("Ross Wightman"),s("OutboundLink")],1),t._v(" has been on a mission to get pretrained weights for the newest Computer Vision models that come out of papers, and compare his results what the papers state themselves. The fantastic results live in his repository "),s("a",{attrs:{href:"https://github.com/rwightman/pytorch-image-models",target:"_blank",rel:"noopener noreferrer"}},[t._v("here"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("For users of the "),s("code",[t._v("fastai")]),t._v(" library, it is a goldmine of models to play with! But how do we use it? Let's set up a basic "),s("code",[t._v("PETs")]),t._v(" problem following the "),s("a",{attrs:{href:"https://walkwithfastai.com/vision.clas.single_label",target:"_blank",rel:"noopener noreferrer"}},[t._v("tutorial"),s("OutboundLink")],1),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("PETS"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npat "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'/([^/]+)_\\d+.*'")]),t._v("\nitem_tfms "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" RandomResizedCrop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("460")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" min_scale"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.75")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ratio"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nbatch_tfms "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("aug_transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" max_warp"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_stats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("imagenet_stats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nbs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),t._v("\npets "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blocks"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ImageBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 get_items"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("get_image_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("RandomSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 get_y"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("RegexLabeller"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pat "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 item_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("item_tfms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 batch_tfms"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("batch_tfms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'images'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("bs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("From here we would normally do something like "),s("code",[t._v("cnn_learner(dls, arch, metrics)")]),t._v(", however we need to do a few things special to work with Ross' framework.")]),t._v(" "),s("p",[s("code",[t._v("fastai")]),t._v(" has a "),s("a",{attrs:{href:"https://docs.fast.ai/vision.learner#create_body",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("create_body")]),s("OutboundLink")],1),t._v(" function, whcih is called during "),s("a",{attrs:{href:"https://docs.fast.ai/vision.learner#cnn_learner",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("cnn_learner")]),s("OutboundLink")],1),t._v(', that will take a model architecuture and slice off the last Linear layer (resulting in a "body" that outputs unpooled features). This function looks like:')]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("create_body")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("arch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_in"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cut"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Cut off the body of a typically pretrained `arch` as determined by `cut`"')]),t._v("\n    model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" arch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pretrained"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    _update_first_layer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_in"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" cut "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        ll "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("children"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        cut "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("next")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("o "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("reversed")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ll"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" has_pool_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v("   "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cut"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("      "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("children"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("cut"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("callable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cut"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" cut"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("                           "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("raise")]),t._v(" NamedError"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"cut must be either integer or a function"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We're going to create our own that plays well")]),t._v(" "),s("blockquote",[s("p",[t._v("Also:notebooks like this are exported as external modules inside of the "),s("code",[t._v("wwf")]),t._v(" library! This one can be found in "),s("code",[t._v("vision.timm")]),t._v(" to be used with your projects!")])]),t._v(" "),s("h4",{staticClass:"doc_header",attrs:{id:"create_timm_body"}},[s("code",[t._v("create_timm_body")]),s("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/walkwithfastai/walkwithfastai.github.io/tree/master/wwf/vision/timm.py#L13"}},[t._v("[source]")])]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("create_timm_body")]),t._v("("),s("strong",[s("code",[t._v("arch")])]),t._v(":"),s("code",[t._v("str")]),t._v(", "),s("strong",[s("code",[t._v("pretrained")])]),t._v("="),s("em",[s("code",[t._v("True")])]),t._v(", "),s("strong",[s("code",[t._v("cut")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("n_in")])]),t._v("="),s("em",[s("code",[t._v("3")])]),t._v(")")])]),t._v(" "),s("p",[t._v("Creates a body from any model in the "),s("code",[t._v("timm")]),t._v(" library.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("create_timm_body")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("arch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cut"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_in"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Creates a body from any model in the `timm` library."')]),t._v("\n    model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" create_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("arch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_classes"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" global_pool"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    _update_first_layer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_in"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" cut "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        ll "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("children"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        cut "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("next")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("o "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("reversed")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ll"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" has_pool_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cut"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("children"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("cut"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("callable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cut"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" cut"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("raise")]),t._v(" NamedError"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"cut must be either integer or function"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("How do we use it? Let's try it out on an "),s("code",[t._v("efficientnet_b3")]),t._v(" architecture (the entire list of supported architectures is found "),s("a",{attrs:{href:"https://github.com/rwightman/pytorch-image-models#models",target:"_blank",rel:"noopener noreferrer"}},[t._v("here"),s("OutboundLink")],1)]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("body "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" create_timm_body"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'efficientnet_b3a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("From here we can calculate the number input features our head needs to have with "),s("a",{attrs:{href:"https://docs.fast.ai/callback.hook#num_features_model",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("num_features_model")]),s("OutboundLink")],1),t._v(".  We'll mutliply this by two since we have two pooling layers, "),s("a",{attrs:{href:"https://docs.fast.ai/layers#AdaptiveConcatPool2d",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("AdaptiveConcatPool2d")]),s("OutboundLink")],1),t._v(" and "),s("code",[t._v("nn.AdaptiveAvgPool2d")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("nf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" num_features_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("body"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" nf\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("3072\n")])])]),s("p",[t._v("And now we can create a head!")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("head "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" create_head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("c"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("To mix them together, we just wrap the two in a "),s("code",[t._v("nn.Sequential")]),t._v(" and we now have a "),s("code",[t._v("PyTorch")]),t._v(" model ready to be trained on:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("net "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("body"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("From here we would pass it onto "),s("a",{attrs:{href:"https://docs.fast.ai/learner#Learner",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Learner")]),s("OutboundLink")],1),t._v(", specifying our "),s("code",[t._v("splitter")]),t._v(" to be the "),s("a",{attrs:{href:"https://docs.fast.ai/vision.learner#default_split",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("default_split")]),s("OutboundLink")],1)]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("default_splitter")]),t._v(" expects the body in "),s("code",[t._v("model[0]")]),t._v(" and the head in "),s("code",[t._v("model[1]")]),t._v(" to split our layer groups")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" net"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("default_split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("To know this all worked properly, we should be able to call "),s("code",[t._v("learn.freeze()")]),t._v(" and check the number of frozen parameters. (You can also call "),s("code",[t._v("learn.summary")]),t._v(" but we are not since it has a lengthy output):")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("freeze"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nunfrozen_params "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("filter")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" p"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" p"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("requires_grad"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nunfrozen_params "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prod"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("p"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" p "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" unfrozen_params"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel_parameters "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("filter")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" p"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" p"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("requires_grad"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfrozen_params "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prod"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("p"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" p "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" model_parameters"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("unfrozen_params"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" frozen_params\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(1686272, 10608936)\n")])])]),s("p",[t._v("Which we can see that only 1.6 million of the 10 million parameters are trainable, so our model is ready for transfer learning!")]),t._v(" "),s("h2",{attrs:{id:"turning-it-all-into-a-function"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#turning-it-all-into-a-function"}},[t._v("#")]),t._v(" Turning it all into a function")]),t._v(" "),s("p",[t._v("Let's make this a bit easier and create something like "),s("a",{attrs:{href:"https://docs.fast.ai/vision.learner#cnn_learner",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("cnn_learner")]),s("OutboundLink")],1),t._v(", but for "),s("code",[t._v("timm")]),t._v("! We'll call it a "),s("RouterLink",{attrs:{to:"/vision.external.timm.html#timm_learner"}},[s("code",[t._v("timm_learner")])]),t._v(". First let's look at and compare what "),s("a",{attrs:{href:"https://docs.fast.ai/vision.learner#cnn_learner",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("cnn_learner")]),s("OutboundLink")],1),t._v(" does internally:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("cnn_learner")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" arch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" loss_func"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cut"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                y_range"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" config"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_out"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" normalize"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("kwargs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Build a convnet style learner from `dls` and `arch`"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" config "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" config "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    meta "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model_meta"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("arch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _default_meta"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" n_out "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" n_out "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_c"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" n_out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" _add_norm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" meta"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" y_range "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y_range'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" y_range "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y_range'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" create_cnn_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("arch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ifnone"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cut"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" meta"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cut'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_range"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("y_range"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" loss_func"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("loss_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ifnone"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("splitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" meta"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'split'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("kwargs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("freeze"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" learn\n")])])]),s("p",[t._v("At first it looks scary, but let's try and read it as best we can:")]),t._v(" "),s("ol",[s("li",[t._v("Grab potential private meta about an architecture we're using")]),t._v(" "),s("li",[t._v("Grab the number of expected outputs")]),t._v(" "),s("li",[t._v("Potentially normalize")]),t._v(" "),s("li",[t._v("Add a "),s("code",[t._v("y_range")])]),t._v(" "),s("li",[t._v("Create a "),s("code",[t._v("cnn_model")]),t._v(" and "),s("a",{attrs:{href:"https://docs.fast.ai/learner#Learner",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Learner")]),s("OutboundLink")],1)]),t._v(" "),s("li",[t._v("Freeze our model")])]),t._v(" "),s("p",[t._v("We're going to make a custom "),s("RouterLink",{attrs:{to:"/vision.external.timm.html#create_timm_model"}},[s("code",[t._v("create_timm_model")])]),t._v(" and "),s("RouterLink",{attrs:{to:"/vision.external.timm.html#timm_learner"}},[s("code",[t._v("timm_learner")])]),t._v(" function to do what we just did above. First, "),s("RouterLink",{attrs:{to:"/vision.external.timm.html#create_timm_model"}},[s("code",[t._v("create_timm_model")])]),t._v(" will model after "),s("a",{attrs:{href:"https://docs.fast.ai/vision.learner#create_cnn_model",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("create_cnn_model")]),s("OutboundLink")],1),t._v(":")],1),t._v(" "),s("h4",{staticClass:"doc_header",attrs:{id:"create_timm_model"}},[s("code",[t._v("create_timm_model")]),s("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/walkwithfastai/walkwithfastai.github.io/tree/master/wwf/vision/timm.py#L25"}},[t._v("[source]")])]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("create_timm_model")]),t._v("("),s("strong",[s("code",[t._v("arch")])]),t._v(":"),s("code",[t._v("str")]),t._v(", "),s("strong",[s("code",[t._v("n_out")])]),t._v(", "),s("strong",[s("code",[t._v("cut")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("pretrained")])]),t._v("="),s("em",[s("code",[t._v("True")])]),t._v(", "),s("strong",[s("code",[t._v("n_in")])]),t._v("="),s("em",[s("code",[t._v("3")])]),t._v(", "),s("strong",[s("code",[t._v("init")])]),t._v("="),s("em",[s("code",[t._v("kaiming_normal_")])]),t._v(", "),s("strong",[s("code",[t._v("custom_head")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("concat_pool")])]),t._v("="),s("em",[s("code",[t._v("True")])]),t._v(", "),s("strong",[t._v("**"),s("code",[t._v("kwargs")])]),t._v(")")])]),t._v(" "),s("p",[t._v("Create custom architecture using "),s("code",[t._v("arch")]),t._v(", "),s("code",[t._v("n_in")]),t._v(" and "),s("code",[t._v("n_out")]),t._v(" from the "),s("code",[t._v("timm")]),t._v(" library")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("create_timm_model")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("arch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cut"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_in"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" init"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("init"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("kaiming_normal_"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" custom_head"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                     concat_pool"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("kwargs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Create custom architecture using `arch`, `n_in` and `n_out` from the `timm` library"')]),t._v("\n    body "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" create_timm_body"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("arch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_in"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" custom_head "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        nf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" num_features_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("body"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("children"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" concat_pool "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        head "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" create_head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" concat_pool"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("concat_pool"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("kwargs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" head "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" custom_head\n    model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("body"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" init "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" apply_init"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" init"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" model\n")])])]),s("p",[t._v("And now for our "),s("RouterLink",{attrs:{to:"/vision.external.timm.html#timm_learner"}},[s("code",[t._v("timm_learner")])]),t._v(":")],1),t._v(" "),s("h4",{staticClass:"doc_header",attrs:{id:"timm_learner"}},[s("code",[t._v("timm_learner")]),s("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/walkwithfastai/walkwithfastai.github.io/tree/master/wwf/vision/timm.py#L41"}},[t._v("[source]")])]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("timm_learner")]),t._v("("),s("strong",[s("code",[t._v("dls")])]),t._v(", "),s("strong",[s("code",[t._v("arch")])]),t._v(":"),s("code",[t._v("str")]),t._v(", "),s("strong",[s("code",[t._v("loss_func")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("pretrained")])]),t._v("="),s("em",[s("code",[t._v("True")])]),t._v(", "),s("strong",[s("code",[t._v("cut")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("splitter")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("y_range")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("config")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("n_out")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("normalize")])]),t._v("="),s("em",[s("code",[t._v("True")])]),t._v(", "),s("strong",[t._v("**"),s("code",[t._v("kwargs")])]),t._v(")")])]),t._v(" "),s("p",[t._v("Build a convnet style learner from "),s("code",[t._v("dls")]),t._v(" and "),s("code",[t._v("arch")]),t._v(" using the "),s("code",[t._v("timm")]),t._v(" library")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("timm_learner")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" arch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" loss_func"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cut"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                y_range"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" config"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_out"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" normalize"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("kwargs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Build a convnet style learner from `dls` and `arch` using the `timm` library"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" config "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" config "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" n_out "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" n_out "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_c"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" n_out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" y_range "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y_range'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" y_range "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y_range'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" create_timm_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("arch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default_split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_range"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("y_range"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" loss_func"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("loss_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" splitter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("default_split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("kwargs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("freeze"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" learn\n")])])]),s("p",[t._v("Let's try it out by making the same model we did a moment ago:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" timm_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'efficientnet_b3a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("And to verify let's look at those parameters one more time:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("unfrozen_params "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("filter")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" p"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" p"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("requires_grad"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nunfrozen_params "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prod"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("p"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" p "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" unfrozen_params"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel_parameters "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("filter")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" p"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" p"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("requires_grad"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfrozen_params "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prod"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("p"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" p "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" model_parameters"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("unfrozen_params"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" frozen_params\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(1686272, 10608936)\n")])])]),s("p",[t._v("They're exactly the same! So now we can utilize any architecture found inside of "),s("code",[t._v("timm")]),t._v(" right away, and we built it in a structure very similar to how native "),s("code",[t._v("fastai")]),t._v(" does it.")]),t._v(" "),s("p",[t._v("To use this module in your own work, simply do:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" wwf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vision"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("timm "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\nlearn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" timm_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'efficientnet_b3a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("error_rate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("{% include note.html content='"),s("code",[t._v("timm")]),t._v(" needs to be installed beforehand' %}")]),t._v(" "),s("h2",{attrs:{id:"model-lookup"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#model-lookup"}},[t._v("#")]),t._v(" Model Lookup")]),t._v(" "),s("p",[t._v("To query various models to see what is available, you should directly use the "),s("code",[t._v("timm")]),t._v(" library.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" timm\n")])])]),s("h3",{attrs:{id:"listing-all-models-available"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#listing-all-models-available"}},[t._v("#")]),t._v(" Listing all models available")]),t._v(" "),s("p",[t._v("One option is to list every model possible:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("timm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("list_models"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("['adv_inception_v3',\n 'cspdarknet53',\n 'cspdarknet53_iabn',\n 'cspresnet50',\n 'cspresnet50d',\n 'cspresnet50w',\n 'cspresnext50',\n 'cspresnext50_iabn',\n 'darknet53',\n 'densenet121']\n")])])]),s("h3",{attrs:{id:"searching-for-models"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#searching-for-models"}},[t._v("#")]),t._v(" Searching for models")]),t._v(" "),s("p",[t._v("You can also query the names of what is available as well, denoted as below:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("timm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("list_models"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'*efficientnet*'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("['efficientnet_b0',\n 'efficientnet_b1',\n 'efficientnet_b1_pruned',\n 'efficientnet_b2',\n 'efficientnet_b2_pruned',\n 'efficientnet_b2a',\n 'efficientnet_b3',\n 'efficientnet_b3_pruned',\n 'efficientnet_b3a',\n 'efficientnet_b4']\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("timm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("list_models"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'*b3a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("['efficientnet_b3a']\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("timm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("list_models"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'resne*t*'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("['resnest14d',\n 'resnest26d',\n 'resnest50d',\n 'resnest50d_1s4x24d',\n 'resnest50d_4s2x40d',\n 'resnest101e',\n 'resnest200e',\n 'resnest269e',\n 'resnet18',\n 'resnet26']\n")])])]),s("h2",{attrs:{id:"some-warnings"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#some-warnings"}},[t._v("#")]),t._v(" Some Warnings")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("Watch for anything with a "),s("code",[t._v("tf_")]),t._v(' prefix. This means the original weights were ported from Google, so it uses manual padding to match TensorFlow\'s "same" padding, which adds GPU overhead and a general slowdown. If possible try to use the non-TF versions of models')])]),t._v(" "),s("li",[s("p",[t._v("HRNet is a bit of a problem-child, so it is the only one not straight-forward to use")])])])])}),[],!1,null,null,null);a.default=e.exports}}]);