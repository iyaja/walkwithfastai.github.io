(window.webpackJsonp=window.webpackJsonp||[]).push([[15],{360:function(t,a,e){"use strict";e.r(a);var s=e(42),n=Object(s.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"single-label-classification-with-computer-vision-beginner"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#single-label-classification-with-computer-vision-beginner"}},[t._v("#")]),t._v(" Single Label Classification with Computer Vision (Beginner)")]),t._v(" "),e("blockquote",[e("p",[t._v("Covering how to perform single-label classification with the PETs dataset")])]),t._v(" "),e("hr"),t._v(" "),e("p",[t._v("This article is also a Jupyter Notebook available to be run from the top down. There will be code snippets that you can then run in any environment. Below are the versions of "),e("code",[t._v("fastai")]),t._v(" and "),e("code",[t._v("fastcore")]),t._v(" currently running at the time of writing this:")]),t._v(" "),e("ul",[e("li",[e("code",[t._v("fastai")]),t._v(": 2.0.14")]),t._v(" "),e("li",[e("code",[t._v("fastcore")]),t._v(": 1.0.11")])]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"introduction"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),e("p",[t._v("This tutorial will cover single-label classification inside of the "),e("code",[t._v("fastai")]),t._v(" library. It will closely follow the lesson 1 notebook from "),e("a",{attrs:{href:"https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0/blob/master/Computer%20Vision/01_Pets.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("A walk with fastai2"),e("OutboundLink")],1),t._v(" and if you wish to watch the lecture video it is available "),e("a",{attrs:{href:"https://www.youtube.com/watch?v=bw4PRyxa-y4&list=PLFDkaGxp5BXDvj3oHoKDgEcH73Aze-eET&index=1&t=4340s",target:"_blank",rel:"noopener noreferrer"}},[t._v("here"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"importing-the-library-and-the-dataset"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#importing-the-library-and-the-dataset"}},[t._v("#")]),t._v(" Importing the Library and the Dataset")]),t._v(" "),e("p",[t._v("First we need to import "),e("code",[t._v("fastai")]),t._v("'s "),e("code",[t._v("vision")]),t._v(" module:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fastai"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vision"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("all")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n")])])]),e("p",[t._v("The "),e("code",[t._v("PETs")]),t._v(" dataset is designed to try and distinguish between 12 species of cats and 25 species of dogs (37 in total). Five years ago the best accuracy was 59% with seperate classifications for parts of the images cropped to related body parts (the head, body, and overall image). Today's neural networks can perform much better on a task such as this, so we will try to achieve a better result using only the one image.")]),t._v(" "),e("p",[t._v("But before anything, we need data!")]),t._v(" "),e("p",[t._v("To do so we will use the "),e("a",{attrs:{href:"https://docs.fast.ai/data.external#untar_data",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("untar_data")]),e("OutboundLink")],1),t._v(" function paired with "),e("a",{attrs:{href:"https://docs.fast.ai/data.external#URLs.PETS",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("URLs.PETS")]),e("OutboundLink")],1),t._v(". This will go ahead and download and extract a "),e("code",[t._v(".gz")]),t._v(" dataset for us. The "),e("a",{attrs:{href:"https://docs.fast.ai/data.external#URLs.PETS",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("URLs.PETS")]),e("OutboundLink")],1),t._v(" denotes the URL where our dataset lives.")]),t._v(" "),e("p",[t._v("We can view the documentation for "),e("a",{attrs:{href:"https://docs.fast.ai/data.external#untar_data",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("untar_data")]),e("OutboundLink")],1),t._v(" by using the "),e("code",[t._v("help")]),t._v(" or "),e("a",{attrs:{href:"https://nbdev.fast.ai/showdoc#doc",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("doc")]),e("OutboundLink")],1),t._v(" functions:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("untar_data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("Help on function untar_data in module fastai.data.external:\n\nuntar_data(url, fname=None, dest=None, c_key='data', force_download=False, extract_func=<function file_extract at 0x7fe4f0427050>)\n    Download `url` to `fname` if `dest` doesn't exist, and un-tgz or unzip to folder `dest`.\n")])])]),e("p",[e("a",{attrs:{href:"https://nbdev.fast.ai/showdoc#doc",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("doc")]),e("OutboundLink")],1),t._v(" will pull it up in a pop-up window (run this notebook to see):")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("doc"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("untar_data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("Along with this if we want to explore the source code for any function, put two "),e("code",[t._v("??")]),t._v(" after the name of the function and a popup window will appear with the source code!")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("untar_data??\n")])])]),e("p",[t._v("Now let's download our dataset:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("path "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("PETS"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("We can see the location of where our data was stored by checking what "),e("code",[t._v("path")]),t._v(" contains:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("path\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("Path('/home/ml1/.fastai/data/oxford-iiit-pet')\n")])])]),e("p",[t._v("To make this notebook reproduceable, we can use "),e("code",[t._v("fastai")]),t._v("'s "),e("a",{attrs:{href:"https://docs.fast.ai/torch_core#set_seed",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("set_seed")]),e("OutboundLink")],1),t._v(" function to ensure every possible source of randomness has the same seed:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("set_seed"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("Now let's look at hour our data was stored:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("path"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ls"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("(#2) [Path('/home/ml1/.fastai/data/oxford-iiit-pet/annotations'),Path('/home/ml1/.fastai/data/oxford-iiit-pet/images')]\n")])])]),e("p",[t._v("We can see a "),e("code",[t._v("images")]),t._v(" and "),e("code",[t._v("annotations")]),t._v(" folder. We'll focus on the "),e("code",[t._v("images")]),t._v(" folder.")]),t._v(" "),e("h2",{attrs:{id:"building-the-dataloaders"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#building-the-dataloaders"}},[t._v("#")]),t._v(" Building the "),e("a",{attrs:{href:"https://docs.fast.ai/data.core#DataLoaders",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("DataLoaders")]),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("To actually train our neural network model we need to first prepare our dataset for "),e("code",[t._v("fastai")]),t._v(" to expect it. This comes in the form of "),e("a",{attrs:{href:"https://docs.fast.ai/data.core#DataLoaders",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("DataLoaders")]),e("OutboundLink")],1),t._v(". We'll show a high-level one-liner usage followed by a "),e("a",{attrs:{href:"https://docs.fast.ai/data.block#DataBlock",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("DataBlock")]),e("OutboundLink")],1),t._v(" example")]),t._v(" "),e("p",[t._v("Since we have the "),e("code",[t._v("path")]),t._v(" to our data, we'll need to extract the filenames of all the images.")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://docs.fast.ai/data.transforms#get_image_files",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("get_image_files")]),e("OutboundLink")],1),t._v(" can do this for us, we simply pass in our "),e("code",[t._v("path")]),t._v(" to where the images are stored:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("fnames "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_image_files"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'images'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("Now if we look at a filename, we will see that the filename itself contains our class label. We can use a Regular Expression to extract it.")]),t._v(" "),e("blockquote",[e("p",[t._v("Do note:throughout this resource you will find multiple ways of completing the same task, so you do not have to follow this example to the letter")])]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pat "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'/([^/]+)_\\d+.*'")]),t._v("\n")])])]),e("p",[t._v("With our pattern that can extract our label, let's move onto some basic transforms:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("item_tfms "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" RandomResizedCrop"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("460")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" min_scale"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.75")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ratio"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nbatch_tfms "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("aug_transforms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("size"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" max_warp"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Normalize"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_stats"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("imagenet_stats"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),e("blockquote",[e("p",[t._v("We intend to use a pretrained model for our task, so we need to normalize our data by the original model's data's statistics")])]),t._v(" "),e("p",[t._v("Along with how many images we want our model to process at one time (a batch size):")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("bs"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),t._v("\n")])])]),e("p",[t._v("Lastly we can build our "),e("a",{attrs:{href:"https://docs.fast.ai/data.core#DataLoaders",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("DataLoaders")]),e("OutboundLink")],1),t._v("! Let's see a one-liner where we pass it all in")]),t._v(" "),e("blockquote",[e("p",[t._v("For those familiar with "),e("code",[t._v("fastai")]),t._v(" v1, this is akin to "),e("code",[t._v("ImageDataBunch")])])]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("dls "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ImageDataLoaders"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_name_re"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fnames"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pat"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                                   batch_tfms"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("batch_tfms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                   item_tfms"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("item_tfms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bs"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("bs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("What about using the "),e("a",{attrs:{href:"https://docs.fast.ai/data.block#DataBlock",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("DataBlock")]),e("OutboundLink")],1),t._v(" API I keep hearing about?")]),t._v(" "),e("p",[t._v("It's a way to formulate a "),e("em",[t._v("blueprint")]),t._v(" of a data pipeline that can be tweaked more than the factory "),e("code",[t._v("xDataLoaders")]),t._v(" methods. The version for our problem looks like:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pets "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataBlock"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blocks"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ImageBlock"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" CategoryBlock"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 get_items"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("get_image_files"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 splitter"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("RandomSplitter"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 get_y"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("RegexLabeller"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pat "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pat"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 item_tfms"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("item_tfms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 batch_tfms"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("batch_tfms"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("Let's break it all down:")]),t._v(" "),e("ul",[e("li",[e("code",[t._v("blocks")]),t._v(":\n"),e("ul",[e("li",[e("a",{attrs:{href:"https://docs.fast.ai/vision.data#ImageBlock",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("ImageBlock")]),e("OutboundLink")],1),t._v(": Our "),e("code",[t._v("x")]),t._v("'s will be images")]),t._v(" "),e("li",[e("a",{attrs:{href:"https://docs.fast.ai/data.block#CategoryBlock",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("CategoryBlock")]),e("OutboundLink")],1),t._v(": Our "),e("code",[t._v("y")]),t._v("s will be a single category label")])])]),t._v(" "),e("li",[e("code",[t._v("get_items")]),t._v(": How we are getting our data. (when doing image problems you will mostly just use "),e("a",{attrs:{href:"https://docs.fast.ai/data.transforms#get_image_files",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("get_image_files")]),e("OutboundLink")],1),t._v(" by default)")]),t._v(" "),e("li",[e("code",[t._v("splitter")]),t._v(": How we want to split our data.\n"),e("ul",[e("li",[e("a",{attrs:{href:"https://docs.fast.ai/data.transforms#RandomSplitter",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("RandomSplitter")]),e("OutboundLink")],1),t._v(": Will randomly split the data with 20% in our validation set (by default), with the other 80% in our training data. We can pass in a percentage and a seed. (we won't be doing the latter as we already set a global seed)")])])]),t._v(" "),e("li",[e("code",[t._v("get_y")]),t._v(": How to extract the labels for our data\n"),e("ul",[e("li",[e("a",{attrs:{href:"https://docs.fast.ai/data.transforms#RegexLabeller",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("RegexLabeller")]),e("OutboundLink")],1),t._v(": Uses a Regex "),e("code",[t._v("pat")]),t._v(" to extract them")])])]),t._v(" "),e("li",[e("code",[t._v("item")]),t._v(" and "),e("code",[t._v("batch")]),t._v(" tfms are our data augmentation")])]),t._v(" "),e("p",[t._v("So now that we have the blueprint we can build our "),e("a",{attrs:{href:"https://docs.fast.ai/data.core#DataLoaders",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("DataLoaders")]),e("OutboundLink")],1),t._v(" by calling the "),e("code",[t._v(".dataloaders")]),t._v(" method. We'll need to pass it a location to find our source images as well as that batch size we defined earlier:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("path_im "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" path"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'images'")]),t._v("\ndls "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pets"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path_im"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bs"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("bs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("h2",{attrs:{id:"looking-at-the-data"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#looking-at-the-data"}},[t._v("#")]),t._v(" Looking at the Data")]),t._v(" "),e("p",[t._v("Now that the "),e("a",{attrs:{href:"https://docs.fast.ai/data.core#DataLoaders",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("DataLoaders")]),e("OutboundLink")],1),t._v(" have been built we can take a peek at the data and a few special bits and pieces about them.")]),t._v(" "),e("p",[t._v("First let's take a look at a batch of data. We can use the "),e("code",[t._v("show_batch")]),t._v(" function and pass in a maximum number of images to show, as well as how large we want them to appear as in our notebooks.")]),t._v(" "),e("blockquote",[e("p",[t._v("By default it will show images from the validation "),e("a",{attrs:{href:"https://docs.fast.ai/data.load#DataLoader",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("DataLoader")]),e("OutboundLink")],1),t._v(". To show data from the training set, use "),e("code",[t._v("dls[0]")]),t._v(" rather than "),e("code",[t._v("dls")])])]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("dls"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("max_n"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" figsize"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[e("img",{attrs:{src:"output_38_0.png",alt:"png"}})]),t._v(" "),e("p",[t._v("If we want to see how many classes we have, and the names of them we can simply call "),e("code",[t._v("dls.vocab")]),t._v(". The first is the number of classes, the second is the names of our classes. You may notice this looks a bit odd, that's because this "),e("a",{attrs:{href:"https://fastcore.fast.ai/foundation#L",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("L")]),e("OutboundLink")],1),t._v(" is a new invention of Jeremy and Sylvian. Essentially it's a Python list taken to the extreme.")]),t._v(" "),e("p",[t._v("Before if we wanted to grab the index for the name of a class (eg. our model output 0 as our class), we would need to use "),e("code",[t._v("data.c2i")]),t._v(" to grab the Class2Index mapping. This is still here, it lives in "),e("code",[t._v("dls.vocab.o2i")]),t._v(" (Object2ID).")]),t._v(" "),e("p",[t._v("It's a dictionary mapping of "),e("code",[t._v("name -> value")]),t._v(", so let's only look at the first five (since we have 37 in total!)")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("k"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dls"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vocab"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("o2i"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("k"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" k "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vocab"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("o2i"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("{'Abyssinian': 0,\n 'Bengal': 1,\n 'Birman': 2,\n 'Bombay': 3,\n 'British_Shorthair': 4}\n")])])]),e("h2",{attrs:{id:"time-to-make-and-train-a-model"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#time-to-make-and-train-a-model"}},[t._v("#")]),t._v(" Time To Make and Train a Model!")]),t._v(" "),e("p",[t._v("We will be using a convolutional neural network backbone and a fully connected head with a single hidden layer as our classifier. Don't worry if thats a bunch of nonsense for now. Right now, just know this: we are piggybacking off of a model to help us classify images into 37 categories.")]),t._v(" "),e("p",[t._v("First we need to make our neural network using a "),e("a",{attrs:{href:"https://docs.fast.ai/learner#Learner",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("Learner")]),e("OutboundLink")],1),t._v(". A "),e("a",{attrs:{href:"https://docs.fast.ai/learner#Learner",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("Learner")]),e("OutboundLink")],1),t._v(" needs (on a basic level):")]),t._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://docs.fast.ai/data.core#DataLoaders",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("DataLoaders")]),e("OutboundLink")],1)]),t._v(" "),e("li",[t._v("An architecture")]),t._v(" "),e("li",[t._v("An evaluation metric (not actually required for training)")]),t._v(" "),e("li",[t._v("A loss function")]),t._v(" "),e("li",[t._v("An optimizer")])]),t._v(" "),e("p",[t._v("We'll also be using "),e("code",[t._v("mixed_precision")]),t._v(" (fp16).")]),t._v(" "),e("p",[t._v("There are many different "),e("a",{attrs:{href:"https://docs.fast.ai/learner#Learner",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("Learner")]),e("OutboundLink")],1),t._v(" cookie-cutters to use based on what problem you are using it for. Since we're doing transfer learning with CNN's, we will use "),e("a",{attrs:{href:"https://docs.fast.ai/vision.learner#cnn_learner",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("cnn_learner")]),e("OutboundLink")],1),t._v(" and a ResNet34:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("learn "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cnn_learner"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" resnet34"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pretrained"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("error_rate"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_fp16"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("Some assumptions and magic is being done here:")]),t._v(" "),e("ul",[e("li",[t._v("Loss function is assumed as classification (from the "),e("a",{attrs:{href:"https://docs.fast.ai/data.core#DataLoaders",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("DataLoaders")]),e("OutboundLink")],1),t._v("), so "),e("a",{attrs:{href:"https://docs.fast.ai/losses#CrossEntropyLossFlat",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("CrossEntropyLossFlat")]),e("OutboundLink")],1),t._v(" is being used")]),t._v(" "),e("li",[t._v("By default the optimizer is Adam")]),t._v(" "),e("li",[t._v("A custom head was added onto our ResNet body, with the body's weights being frozen")])]),t._v(" "),e("p",[t._v("Now we can train it! We will train for one cycle through all our data:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("learn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("table",{staticClass:"dataframe",attrs:{border:"1"}},[e("thead",[e("tr",{staticStyle:{"text-align":"left"}},[e("th",[t._v("epoch")]),t._v(" "),e("th",[t._v("train_loss")]),t._v(" "),e("th",[t._v("valid_loss")]),t._v(" "),e("th",[t._v("error_rate")]),t._v(" "),e("th",[t._v("time")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("0")]),t._v(" "),e("td",[t._v("1.255419")]),t._v(" "),e("td",[t._v("0.317409")]),t._v(" "),e("td",[t._v("0.102165")]),t._v(" "),e("td",[t._v("00:26")])])])]),t._v(" "),e("p",[t._v("Afterwards we can "),e("em",[t._v("unfreeze")]),t._v(" those frozen weights and train a little more. We'll utilize the "),e("a",{attrs:{href:"https://docs.fast.ai/callback.schedule#Learner.lr_find",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("Learner.lr_find")]),e("OutboundLink")],1),t._v(" function to help us decide a good learning rate:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("learn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lr_find"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("SuggestedLRs(lr_min=0.00043651582673192023, lr_steep=1.9054607491852948e-06)\n")])])]),e("p",[e("img",{attrs:{src:"output_49_2.png",alt:"png"}})]),t._v(" "),e("p",[t._v("Alright so if we look here, we don't really start seeing a spike in our losses until we get close to 1e-2, so a good section to train on is between 1e-4 and 1e-3, so we'll do that!")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("learn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unfreeze"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_one_cycle"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lr_max"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("slice")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("table",{staticClass:"dataframe",attrs:{border:"1"}},[e("thead",[e("tr",{staticStyle:{"text-align":"left"}},[e("th",[t._v("epoch")]),t._v(" "),e("th",[t._v("train_loss")]),t._v(" "),e("th",[t._v("valid_loss")]),t._v(" "),e("th",[t._v("error_rate")]),t._v(" "),e("th",[t._v("time")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("0")]),t._v(" "),e("td",[t._v("0.627381")]),t._v(" "),e("td",[t._v("0.798733")]),t._v(" "),e("td",[t._v("0.234100")]),t._v(" "),e("td",[t._v("00:27")])]),t._v(" "),e("tr",[e("td",[t._v("1")]),t._v(" "),e("td",[t._v("0.556349")]),t._v(" "),e("td",[t._v("0.454100")]),t._v(" "),e("td",[t._v("0.144114")]),t._v(" "),e("td",[t._v("00:27")])]),t._v(" "),e("tr",[e("td",[t._v("2")]),t._v(" "),e("td",[t._v("0.309109")]),t._v(" "),e("td",[t._v("0.257731")]),t._v(" "),e("td",[t._v("0.083897")]),t._v(" "),e("td",[t._v("00:27")])]),t._v(" "),e("tr",[e("td",[t._v("3")]),t._v(" "),e("td",[t._v("0.146716")]),t._v(" "),e("td",[t._v("0.221774")]),t._v(" "),e("td",[t._v("0.072395")]),t._v(" "),e("td",[t._v("00:27")])])])]),t._v(" "),e("p",[t._v("And now we have a fully trained model! At the start we set a goal: to beat 59% accuracy usin gonly the image inputs.")]),t._v(" "),e("p",[t._v("We more than succeeded, achieving 94% accuracy while training for only five epochs!")]),t._v(" "),e("p",[t._v("To use such a model in production, examples of "),e("code",[t._v("learn.predict")]),t._v(" and "),e("code",[t._v("learn.get_preds")]),t._v(" with "),e("code",[t._v("test_dl")]),t._v(" are shown below:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("clas"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" clas_idx"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" probs "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fnames"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" clas\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("'Birman'\n")])])]),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("test_dl "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dls"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test_dl"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fnames"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npreds "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_preds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dl"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("test_dl"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("To get the class names decoded we can do the following:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("class_idxs "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" preds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("argmax"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dim"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nres "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("dls"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vocab"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("c"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" c "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" class_idxs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("res"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("['Russian_Blue',\n 'Bombay',\n 'Abyssinian',\n 'Persian',\n 'Egyptian_Mau',\n 'Russian_Blue',\n 'Russian_Blue',\n 'Bengal',\n 'Abyssinian',\n 'Egyptian_Mau']\n")])])])])}),[],!1,null,null,null);a.default=n.exports}}]);