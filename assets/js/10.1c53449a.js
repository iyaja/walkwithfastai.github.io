(window.webpackJsonp=window.webpackJsonp||[]).push([[10],{363:function(t,a,s){"use strict";s.r(a);var e=s(42),n=Object(e.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"tabular-binary-classification-beginner"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tabular-binary-classification-beginner"}},[t._v("#")]),t._v(" Tabular Binary Classification (Beginner)")]),t._v(" "),s("blockquote",[s("p",[t._v("Introducing the Tabular API with an example problem")])]),t._v(" "),s("hr"),t._v(" "),s("p",[t._v("This article is also a Jupyter Notebook available to be run from the top down. There will be code snippets that you can then run in any environment. Below are the versions of "),s("code",[t._v("fastai")]),t._v(" and "),s("code",[t._v("fastcore")]),t._v(" currently running at the time of writing this:")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("fastai")]),t._v(": 2.0.14")]),t._v(" "),s("li",[s("code",[t._v("fastcore")]),t._v(": 1.0.11")])]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"binary-classification"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#binary-classification"}},[t._v("#")]),t._v(" Binary Classification")]),t._v(" "),s("p",[t._v("In this example we will be walking through the "),s("code",[t._v("fastai")]),t._v(" tabular API to perform binary classification on the Salary dataset.")]),t._v(" "),s("p",[t._v("This notebook can run along side the first tabular lesson from Walk with fastai2, shown "),s("a",{attrs:{href:"https://www.youtube.com/watch?v=liTHAhdl1cQ&list=PLFDkaGxp5BXDvj3oHoKDgEcH73Aze-eET&index=9&t=430s",target:"_blank",rel:"noopener noreferrer"}},[t._v("here"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("First we need to call the tabular module:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fastai"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tabular"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("all")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n")])])]),s("p",[t._v("And grab our dataset:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ADULT_SAMPLE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("If we look at the contents of our folder, we will find our data lives in "),s("code",[t._v("adult.csv")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(#3) [Path('/home/ml1/.fastai/data/adult_sample/models'),Path('/home/ml1/.fastai/data/adult_sample/export.pkl'),Path('/home/ml1/.fastai/data/adult_sample/adult.csv')]\n")])])]),s("p",[t._v("We'll go ahead and open it in "),s("code",[t._v("Pandas")]),t._v(" and take a look:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("df "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'adult.csv'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("age")]),t._v(" "),s("th",[t._v("workclass")]),t._v(" "),s("th",[t._v("fnlwgt")]),t._v(" "),s("th",[t._v("education")]),t._v(" "),s("th",[t._v("education-num")]),t._v(" "),s("th",[t._v("marital-status")]),t._v(" "),s("th",[t._v("occupation")]),t._v(" "),s("th",[t._v("relationship")]),t._v(" "),s("th",[t._v("race")]),t._v(" "),s("th",[t._v("sex")]),t._v(" "),s("th",[t._v("capital-gain")]),t._v(" "),s("th",[t._v("capital-loss")]),t._v(" "),s("th",[t._v("hours-per-week")]),t._v(" "),s("th",[t._v("native-country")]),t._v(" "),s("th",[t._v("salary")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("49")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("101320")]),t._v(" "),s("td",[t._v("Assoc-acdm")]),t._v(" "),s("td",[t._v("12.0")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("1902")]),t._v(" "),s("td",[t._v("40")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("44")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("236746")]),t._v(" "),s("td",[t._v("Masters")]),t._v(" "),s("td",[t._v("14.0")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("Exec-managerial")]),t._v(" "),s("td",[t._v("Not-in-family")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("Male")]),t._v(" "),s("td",[t._v("10520")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("45")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("38")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("96185")]),t._v(" "),s("td",[t._v("HS-grad")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Unmarried")]),t._v(" "),s("td",[t._v("Black")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("32")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("38")]),t._v(" "),s("td",[t._v("Self-emp-inc")]),t._v(" "),s("td",[t._v("112847")]),t._v(" "),s("td",[t._v("Prof-school")]),t._v(" "),s("td",[t._v("15.0")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Prof-specialty")]),t._v(" "),s("td",[t._v("Husband")]),t._v(" "),s("td",[t._v("Asian-Pac-Islander")]),t._v(" "),s("td",[t._v("Male")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("40")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("42")]),t._v(" "),s("td",[t._v("Self-emp-not-inc")]),t._v(" "),s("td",[t._v("82297")]),t._v(" "),s("td",[t._v("7th-8th")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Other-service")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("Black")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("50")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v("<50k")])])])])]),t._v(" "),s("h2",{attrs:{id:"tabularpandas"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tabularpandas"}},[t._v("#")]),t._v(" TabularPandas")]),t._v(" "),s("p",[s("code",[t._v("fastai")]),t._v(" has a new way of dealing with tabular data by utilizing a "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" object. It expects some dataframe, some "),s("code",[t._v("procs")]),t._v(", "),s("code",[t._v("cat_names")]),t._v(", "),s("code",[t._v("cont_names")]),t._v(", "),s("code",[t._v("y_names")]),t._v(", "),s("code",[t._v("y_block")]),t._v(", and some "),s("code",[t._v("splits")]),t._v(". We'll walk through all of them")]),t._v(" "),s("p",[t._v("First we need to grab our categorical and continuous variables, along with how we want to process our data.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("cat_names "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'workclass'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'marital-status'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'occupation'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relationship'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'race'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ncont_names "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'age'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fnlwgt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education-num'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("p",[t._v("When we pre-process tabular data with "),s("code",[t._v("fastai")]),t._v(", we do one or more of three transforms:")]),t._v(" "),s("ul",[s("li",[s("RouterLink",{attrs:{to:"/tab.stats.html#Categorify"}},[s("code",[t._v("Categorify")])])],1),t._v(" "),s("li",[s("RouterLink",{attrs:{to:"/tab.stats.html#FillMissing"}},[s("code",[t._v("FillMissing")])])],1),t._v(" "),s("li",[s("a",{attrs:{href:"https://docs.fast.ai/data.transforms#Normalize",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Normalize")]),s("OutboundLink")],1)])]),t._v(" "),s("h3",{attrs:{id:"categorify"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#categorify"}},[t._v("#")]),t._v(" Categorify")]),t._v(" "),s("p",[s("RouterLink",{attrs:{to:"/tab.stats.html#Categorify"}},[s("code",[t._v("Categorify")])]),t._v(" will transform columns that are in your "),s("code",[t._v("cat_names")]),t._v(" into that type, along with label encoding our categorical data.")],1),t._v(" "),s("p",[t._v("First we'll make an instance of it:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("cat "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Categorify"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("And now let's try transforming a dataframe")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularPandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We can then extract that transform from "),s("code",[t._v("to.procs.categorify")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("cats "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("procs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("categorify\n")])])]),s("p",[t._v("Let's take a look at the categories:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("cats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relationship'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(#7) ['#na#',' Husband',' Not-in-family',' Other-relative',' Own-child',' Unmarried',' Wife']\n")])])]),s("p",[t._v("We can see that it added a "),s("code",[t._v("#na#")]),t._v("category. Let's look at the actual column:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("max_n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("workclass")]),t._v(" "),s("th",[t._v("education")]),t._v(" "),s("th",[t._v("marital-status")]),t._v(" "),s("th",[t._v("occupation")]),t._v(" "),s("th",[t._v("relationship")]),t._v(" "),s("th",[t._v("race")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("Assoc-acdm")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("#na#")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("White")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("Masters")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("Exec-managerial")]),t._v(" "),s("td",[t._v("Not-in-family")]),t._v(" "),s("td",[t._v("White")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("HS-grad")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("#na#")]),t._v(" "),s("td",[t._v("Unmarried")]),t._v(" "),s("td",[t._v("Black")])])])]),t._v(" "),s("p",[t._v("We can see now, for example, that "),s("code",[t._v("occupation")]),t._v(" got returned a "),s("code",[t._v("#na#")]),t._v("value (as it was missing)")]),t._v(" "),s("p",[t._v("If we call "),s("code",[t._v("to.cats")]),t._v(" we can see our one-hot encoded variables:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("workclass")]),t._v(" "),s("th",[t._v("education")]),t._v(" "),s("th",[t._v("marital-status")]),t._v(" "),s("th",[t._v("occupation")]),t._v(" "),s("th",[t._v("relationship")]),t._v(" "),s("th",[t._v("race")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("8")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("6")]),t._v(" "),s("td",[t._v("5")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("13")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v("5")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("12")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("5")]),t._v(" "),s("td",[t._v("3")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("6")]),t._v(" "),s("td",[t._v("15")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("11")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("2")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("7")]),t._v(" "),s("td",[t._v("6")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v("9")]),t._v(" "),s("td",[t._v("6")]),t._v(" "),s("td",[t._v("3")])])])])]),t._v(" "),s("h3",{attrs:{id:"normalize"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#normalize"}},[t._v("#")]),t._v(" Normalize")]),t._v(" "),s("p",[t._v("To properly work with our numerical columns, we need to show a relationship between them all that our model can understand. This is commonly done through Normalization, where we scale the data between -1 and 1, and compute a "),s("code",[t._v("z-score")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("norm "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Let's make another "),s("code",[t._v("to")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularPandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" norm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cont_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cont_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("norms "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("procs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("normalize\n")])])]),s("p",[t._v("And take a closer look.")]),t._v(" "),s("p",[t._v("We can grab the means and standard deviations like so:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("norms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("means\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("{'age': 38.58164675532078,\n 'fnlwgt': 189778.36651208502,\n 'education-num': 10.079815426825466}\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("norms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stds\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("{'age': 13.64022319230403,\n 'fnlwgt': 105548.3568809906,\n 'education-num': 2.5729591440613078}\n")])])]),s("p",[t._v("And we can also call "),s("code",[t._v("to.conts")]),t._v(" to take a look at our transformed data:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("age")]),t._v(" "),s("th",[t._v("fnlwgt")]),t._v(" "),s("th",[t._v("education-num")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("0.763796")]),t._v(" "),s("td",[t._v("-0.838084")]),t._v(" "),s("td",[t._v("0.746294")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("0.397233")]),t._v(" "),s("td",[t._v("0.444987")]),t._v(" "),s("td",[t._v("1.523609")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("-0.042642")]),t._v(" "),s("td",[t._v("-0.886734")]),t._v(" "),s("td",[t._v("NaN")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("-0.042642")]),t._v(" "),s("td",[t._v("-0.728873")]),t._v(" "),s("td",[t._v("1.912267")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("0.250608")]),t._v(" "),s("td",[t._v("-1.018314")]),t._v(" "),s("td",[t._v("NaN")])])])])]),t._v(" "),s("h3",{attrs:{id:"fillmissing"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#fillmissing"}},[t._v("#")]),t._v(" FillMissing")]),t._v(" "),s("p",[t._v("Now the last thing we need to do is take care of any missing values in our "),s("strong",[t._v("continuous")]),t._v(" variables (we have a special "),s("code",[t._v("#na#")]),t._v(" for categorical data already). We have three strategies we can use:")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("median")])]),t._v(" "),s("li",[s("code",[t._v("mode")])]),t._v(" "),s("li",[s("code",[t._v("constant")])])]),t._v(" "),s("p",[t._v("By default it uses "),s("code",[t._v("median")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("fm "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" FillMissing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fill_strategy"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("FillStrategy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("median"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We'll recreate another "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularPandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cont_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cont_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Let's look at those missing values in the first few rows:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("age")]),t._v(" "),s("th",[t._v("fnlwgt")]),t._v(" "),s("th",[t._v("education-num")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("49")]),t._v(" "),s("td",[t._v("101320")]),t._v(" "),s("td",[t._v("12.0")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("44")]),t._v(" "),s("td",[t._v("236746")]),t._v(" "),s("td",[t._v("14.0")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("38")]),t._v(" "),s("td",[t._v("96185")]),t._v(" "),s("td",[t._v("10.0")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("38")]),t._v(" "),s("td",[t._v("112847")]),t._v(" "),s("td",[t._v("15.0")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("42")]),t._v(" "),s("td",[t._v("82297")]),t._v(" "),s("td",[t._v("10.0")])])])])]),t._v(" "),s("p",[s("strong",[t._v("But wait!")]),t._v(" There's more!")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat_names\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(#1) ['education-num_na']\n")])])]),s("p",[t._v("We have categorical values?! Yes!")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("education-num_na")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("False")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("False")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("True")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("False")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("True")])])])])]),t._v(" "),s("p",[t._v("We now have an additional boolean value based on if the value was missing or not too!")]),t._v(" "),s("h2",{attrs:{id:"the-dataloaders"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#the-dataloaders"}},[t._v("#")]),t._v(" The DataLoaders")]),t._v(" "),s("p",[t._v("Now let's build our "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" for classifying. We're also going to want to split our data and declare our "),s("code",[t._v("y_names")]),t._v(" too:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("splits "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" RandomSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("range_of"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsplits\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("((#26049) [18724,19703,4062,9102,28824,4054,5833,16188,2731,28161...],\n (#6512) [24465,976,1726,10178,4740,3920,32288,26018,20274,9660...])\n")])])]),s("p",[t._v("What is "),s("a",{attrs:{href:"https://fastcore.fast.ai/utils#range_of",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("range_of")]),s("OutboundLink")],1),t._v("?")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("range_of"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("([0, 1, 2, 3, 4], 32561)\n")])])]),s("p",[t._v("It's a list of total index's in our "),s("code",[t._v("DataFrame")])]),t._v(" "),s("p",[t._v("We'll use all our "),s("code",[t._v("cat")]),t._v(" and "),s("code",[t._v("cont")]),t._v(" names, the "),s("code",[t._v("procs")]),t._v(", declare a "),s("code",[t._v("y_name")]),t._v(", and finally specify a single-label classification problem with "),s("a",{attrs:{href:"https://docs.fast.ai/data.block#CategoryBlock",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("CategoryBlock")]),s("OutboundLink")],1)]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("cat_names "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'workclass'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'marital-status'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'occupation'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relationship'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'race'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ncont_names "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'age'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fnlwgt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education-num'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nprocs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Categorify"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FillMissing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ny_names "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'salary'")]),t._v("\ny_block "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" CategoryBlock"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Now that we have everything declared, let's build our "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularPandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" procs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("procs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cat_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cont_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cont_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   y_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("y_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_block"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("y_block"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" splits"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("splits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("And now we can build the "),s("a",{attrs:{href:"https://docs.fast.ai/data.core#DataLoaders",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("DataLoaders")]),s("OutboundLink")],1),t._v(". We can do this one of two ways, first just calling "),s("code",[t._v("to.dataloaders()")]),t._v(" on our data:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Or we can create the "),s("a",{attrs:{href:"https://docs.fast.ai/data.core#DataLoaders",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("DataLoaders")]),s("OutboundLink")],1),t._v(" ourselves (a train and valid). One great reason to do this this way is we can pass in different batch sizes into each "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabDataLoader",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabDataLoader")]),s("OutboundLink")],1),t._v(", along with changing options like "),s("code",[t._v("shuffle")]),t._v(" and "),s("code",[t._v("drop_last")])]),t._v(" "),s("p",[t._v("So how do we use it? Our train and validation data live in to.train and to.valid right now, so we specify that along with our options. When you make a training DataLoader, you want "),s("code",[t._v("shuffle")]),t._v(" to be "),s("code",[t._v("True")]),t._v(" and "),s("code",[t._v("drop_last")]),t._v(" to be "),s("code",[t._v("True")]),t._v(" (so we drop the last incomplete batch)")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("trn_dl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabDataLoader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shuffle"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" drop_last"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nval_dl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabDataLoader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("valid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Now we can make some "),s("a",{attrs:{href:"https://docs.fast.ai/data.core#DataLoaders",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("DataLoaders")]),s("OutboundLink")],1),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataLoaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("trn_dl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val_dl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("And show a batch of data:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("workclass")]),t._v(" "),s("th",[t._v("education")]),t._v(" "),s("th",[t._v("marital-status")]),t._v(" "),s("th",[t._v("occupation")]),t._v(" "),s("th",[t._v("relationship")]),t._v(" "),s("th",[t._v("race")]),t._v(" "),s("th",[t._v("education-num_na")]),t._v(" "),s("th",[t._v("age")]),t._v(" "),s("th",[t._v("fnlwgt")]),t._v(" "),s("th",[t._v("education-num")]),t._v(" "),s("th",[t._v("salary")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("11th")]),t._v(" "),s("td",[t._v("Never-married")]),t._v(" "),s("td",[t._v("Sales")]),t._v(" "),s("td",[t._v("Own-child")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("17.0")]),t._v(" "),s("td",[t._v("200199.000263")]),t._v(" "),s("td",[t._v("7.0")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("HS-grad")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("Exec-managerial")]),t._v(" "),s("td",[t._v("Not-in-family")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("36.0")]),t._v(" "),s("td",[t._v("256635.997971")]),t._v(" "),s("td",[t._v("9.0")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("HS-grad")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Transport-moving")]),t._v(" "),s("td",[t._v("Husband")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("44.0")]),t._v(" "),s("td",[t._v("172032.000108")]),t._v(" "),s("td",[t._v("9.0")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("Self-emp-not-inc")]),t._v(" "),s("td",[t._v("5th-6th")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Craft-repair")]),t._v(" "),s("td",[t._v("Husband")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("44.0")]),t._v(" "),s("td",[t._v("112506.998184")]),t._v(" "),s("td",[t._v("3.0")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("Local-gov")]),t._v(" "),s("td",[t._v("Some-college")]),t._v(" "),s("td",[t._v("Never-married")]),t._v(" "),s("td",[t._v("Exec-managerial")]),t._v(" "),s("td",[t._v("Not-in-family")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("40.0")]),t._v(" "),s("td",[t._v("74949.002528")]),t._v(" "),s("td",[t._v("10.0")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("5")]),t._v(" "),s("td",[t._v("?")]),t._v(" "),s("td",[t._v("10th")]),t._v(" "),s("td",[t._v("Never-married")]),t._v(" "),s("td",[t._v("?")]),t._v(" "),s("td",[t._v("Own-child")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("17.0")]),t._v(" "),s("td",[t._v("138506.999270")]),t._v(" "),s("td",[t._v("6.0")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("6")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("Assoc-voc")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Prof-specialty")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("28.0")]),t._v(" "),s("td",[t._v("247819.002902")]),t._v(" "),s("td",[t._v("11.0")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("7")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("Masters")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Exec-managerial")]),t._v(" "),s("td",[t._v("Husband")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("37.0")]),t._v(" "),s("td",[t._v("112496.998941")]),t._v(" "),s("td",[t._v("14.0")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("8")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("HS-grad")]),t._v(" "),s("td",[t._v("Separated")]),t._v(" "),s("td",[t._v("Handlers-cleaners")]),t._v(" "),s("td",[t._v("Not-in-family")]),t._v(" "),s("td",[t._v("Black")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("41.0")]),t._v(" "),s("td",[t._v("215479.000444")]),t._v(" "),s("td",[t._v("9.0")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("9")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("Assoc-voc")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("Sales")]),t._v(" "),s("td",[t._v("Own-child")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("31.0")]),t._v(" "),s("td",[t._v("163302.999641")]),t._v(" "),s("td",[t._v("11.0")]),t._v(" "),s("td",[t._v("<50k")])])])]),t._v(" "),s("blockquote",[s("p",[t._v("Why can we do the "),s("code",[t._v(".dataloaders()")]),t._v("? Because "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" itself is actually a set of "),s("code",[t._v("TabDataLoaders")]),t._v("! See below for a comparison test:")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_dbunch_type "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_dbunch_type\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("True\n")])])]),s("h2",{attrs:{id:"tabular-learner-and-training-a-model"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tabular-learner-and-training-a-model"}},[t._v("#")]),t._v(" Tabular Learner and Training a Model")]),t._v(" "),s("p",[t._v("Now we can build our "),s("a",{attrs:{href:"https://docs.fast.ai/learner#Learner",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Learner")]),s("OutboundLink")],1),t._v("! But what's special about a tabular neural network?")]),t._v(" "),s("h3",{attrs:{id:"categorical-variables"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#categorical-variables"}},[t._v("#")]),t._v(" Categorical Variables")]),t._v(" "),s("p",[t._v("When dealing with our categorical data, we create what is called an "),s("strong",[t._v("embedding matrix")]),t._v(". This allows for a higher dimentionality for relationships between the different categorical cardinalities. Finding the best size ratio was done through experiments by Jeremy on the Rossmann dataset")]),t._v(" "),s("p",[t._v('This "rule of thumb" is to use either a maximum embedding space of 600, or 1.6 times the cardinality raised to the 0.56, or written out as:')]),t._v(" "),s("p",[t._v("{% raw %}\n$$min(600, (1.6 * {var.nunique)}^{0.56})$$\n{% endraw %}")]),t._v(" "),s("p",[t._v("Let's calculate these embedding sizes for our model to take a look-see:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("emb_szs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_emb_sz"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" emb_szs\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("[(10, 6), (17, 8), (8, 5), (16, 8), (7, 5), (6, 4), (3, 3)]\n")])])]),s("p",[t._v("If we want to see what each one aligns to, let's look at the order of "),s("code",[t._v("cat_names")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat_names\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(#7) ['workclass','education','marital-status','occupation','relationship','race','education-num_na']\n")])])]),s("p",[t._v("Let's specifically look at "),s("code",[t._v("workclass")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'workclass'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nunique"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("9\n")])])]),s("p",[t._v("If you notice, we had "),s("code",[t._v("10")]),t._v(" there, this is to take one more column for any missing categorical values that may show")]),t._v(" "),s("h3",{attrs:{id:"numerical-variables"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#numerical-variables"}},[t._v("#")]),t._v(" Numerical Variables")]),t._v(" "),s("p",[t._v("Numericals we can simply pass in how many there are to the model:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("cont_len "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cont_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" cont_len\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("3\n")])])]),s("p",[t._v("And now we have all the pieces we need to build a "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.model#TabularModel",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularModel")]),s("OutboundLink")],1),t._v("!")]),t._v(" "),s("h3",{attrs:{id:"tabularmodel"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tabularmodel"}},[t._v("#")]),t._v(" TabularModel")]),t._v(" "),s("p",[t._v("What makes this model a little different is our batches is actually two inputs:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("batch "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("one_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("3\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("\nbatch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(tensor([ 5, 12,  5,  5,  2,  5,  1]), tensor([-0.1858, -0.4134, -0.4253]))\n")])])]),s("p",[t._v("With the first being our categorical variables and the second being our numericals.")]),t._v(" "),s("p",[t._v("Now let's make our model. We'll want our size of our embeddings, the number of continuous variables, the number of outputs, and how large and how many fully connected layers we want to use:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("net "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularModel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("emb_szs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cont_len"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Let's see it's architecture:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("net\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("TabularModel(\n  (embeds): ModuleList(\n    (0): Embedding(10, 6)\n    (1): Embedding(17, 8)\n    (2): Embedding(8, 5)\n    (3): Embedding(16, 8)\n    (4): Embedding(7, 5)\n    (5): Embedding(6, 4)\n    (6): Embedding(3, 3)\n  )\n  (emb_drop): Dropout(p=0.0, inplace=False)\n  (bn_cont): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layers): Sequential(\n    (0): LinBnDrop(\n      (0): BatchNorm1d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=42, out_features=200, bias=False)\n      (2): ReLU(inplace=True)\n    )\n    (1): LinBnDrop(\n      (0): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=200, out_features=100, bias=False)\n      (2): ReLU(inplace=True)\n    )\n    (2): LinBnDrop(\n      (0): Linear(in_features=100, out_features=2, bias=True)\n    )\n  )\n)\n")])])]),s("h3",{attrs:{id:"tabular-learner"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tabular-learner"}},[t._v("#")]),t._v(" tabular_learner")]),t._v(" "),s("p",[t._v("Now that we know the background, let's build our model a little bit faster and generate a "),s("a",{attrs:{href:"https://docs.fast.ai/learner#Learner",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Learner")]),s("OutboundLink")],1),t._v(" too:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tabular_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("And now we can fit!")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lr_find"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("SuggestedLRs(lr_min=0.0013182567432522773, lr_steep=1.3182567358016968)\n")])])]),s("p",[s("img",{attrs:{src:"output_89_2.png",alt:"png"}})]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.373488")]),t._v(" "),s("td",[t._v("0.362226")]),t._v(" "),s("td",[t._v("0.841523")]),t._v(" "),s("td",[t._v("00:03")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.367182")]),t._v(" "),s("td",[t._v("0.354484")]),t._v(" "),s("td",[t._v("0.838759")]),t._v(" "),s("td",[t._v("00:03")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.368737")]),t._v(" "),s("td",[t._v("0.355205")]),t._v(" "),s("td",[t._v("0.834306")]),t._v(" "),s("td",[t._v("00:03")])])])]),t._v(" "),s("p",[t._v("Can we speed this up a little? Yes we can! The more you can load into a batch, the faster you can process the data. This is a careful balance, for tabular data I go to a maximum of 4096 rows per batch if the dataset is large enough for a decent number of batches:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1024")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tabular_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.389950")]),t._v(" "),s("td",[t._v("0.407716")]),t._v(" "),s("td",[t._v("0.812500")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.366711")]),t._v(" "),s("td",[t._v("0.350281")]),t._v(" "),s("td",[t._v("0.839681")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.357688")]),t._v(" "),s("td",[t._v("0.355638")]),t._v(" "),s("td",[t._v("0.836763")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("p",[t._v("We can see we fit very quickly, but it didn't fit quite as well (there is a trade-off):")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataloaders"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4096")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tabular_learner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"left"}},[s("th",[t._v("epoch")]),t._v(" "),s("th",[t._v("train_loss")]),t._v(" "),s("th",[t._v("valid_loss")]),t._v(" "),s("th",[t._v("accuracy")]),t._v(" "),s("th",[t._v("time")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0.449081")]),t._v(" "),s("td",[t._v("0.503046")]),t._v(" "),s("td",[t._v("0.772267")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("0.407749")]),t._v(" "),s("td",[t._v("0.465814")]),t._v(" "),s("td",[t._v("0.758446")]),t._v(" "),s("td",[t._v("00:00")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("0.387042")]),t._v(" "),s("td",[t._v("0.445514")]),t._v(" "),s("td",[t._v("0.776720")]),t._v(" "),s("td",[t._v("00:00")])])])]),t._v(" "),s("h2",{attrs:{id:"inference"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#inference"}},[t._v("#")]),t._v(" Inference")]),t._v(" "),s("p",[t._v("Now let's look at how we can perform inference. To do predictions we can use "),s("code",[t._v("fastai")]),t._v("'s in-house "),s("code",[t._v("learn.predict")]),t._v(" for individual rows, and "),s("code",[t._v("get_preds")]),t._v(" + "),s("code",[t._v("test_dl")]),t._v(" for batches of predictions:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("row"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" probs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("row"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("workclass")]),t._v(" "),s("th",[t._v("education")]),t._v(" "),s("th",[t._v("marital-status")]),t._v(" "),s("th",[t._v("occupation")]),t._v(" "),s("th",[t._v("relationship")]),t._v(" "),s("th",[t._v("race")]),t._v(" "),s("th",[t._v("education-num_na")]),t._v(" "),s("th",[t._v("age")]),t._v(" "),s("th",[t._v("fnlwgt")]),t._v(" "),s("th",[t._v("education-num")]),t._v(" "),s("th",[t._v("salary")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("Assoc-acdm")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("#na#")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("49.0")]),t._v(" "),s("td",[t._v("101320.00007")]),t._v(" "),s("td",[t._v("12.0")]),t._v(" "),s("td",[t._v("<50k")])])])]),t._v(" "),s("p",[t._v("Now let's try "),s("code",[t._v("test_dl")]),t._v(". There's something special we can do here too:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test_dl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Let's look at a batch:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show_batch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("workclass")]),t._v(" "),s("th",[t._v("education")]),t._v(" "),s("th",[t._v("marital-status")]),t._v(" "),s("th",[t._v("occupation")]),t._v(" "),s("th",[t._v("relationship")]),t._v(" "),s("th",[t._v("race")]),t._v(" "),s("th",[t._v("education-num_na")]),t._v(" "),s("th",[t._v("age")]),t._v(" "),s("th",[t._v("fnlwgt")]),t._v(" "),s("th",[t._v("education-num")]),t._v(" "),s("th",[t._v("salary")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("Assoc-acdm")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("#na#")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("49.0")]),t._v(" "),s("td",[t._v("101320.000070")]),t._v(" "),s("td",[t._v("12.0")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("Masters")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("Exec-managerial")]),t._v(" "),s("td",[t._v("Not-in-family")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("44.0")]),t._v(" "),s("td",[t._v("236745.999937")]),t._v(" "),s("td",[t._v("14.0")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("HS-grad")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("#na#")]),t._v(" "),s("td",[t._v("Unmarried")]),t._v(" "),s("td",[t._v("Black")]),t._v(" "),s("td",[t._v("True")]),t._v(" "),s("td",[t._v("38.0")]),t._v(" "),s("td",[t._v("96185.001851")]),t._v(" "),s("td",[t._v("10.0")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("Self-emp-inc")]),t._v(" "),s("td",[t._v("Prof-school")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Prof-specialty")]),t._v(" "),s("td",[t._v("Husband")]),t._v(" "),s("td",[t._v("Asian-Pac-Islander")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("38.0")]),t._v(" "),s("td",[t._v("112846.997614")]),t._v(" "),s("td",[t._v("15.0")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("Self-emp-not-inc")]),t._v(" "),s("td",[t._v("7th-8th")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Other-service")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("Black")]),t._v(" "),s("td",[t._v("True")]),t._v(" "),s("td",[t._v("42.0")]),t._v(" "),s("td",[t._v("82296.994924")]),t._v(" "),s("td",[t._v("10.0")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("5")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("HS-grad")]),t._v(" "),s("td",[t._v("Never-married")]),t._v(" "),s("td",[t._v("Handlers-cleaners")]),t._v(" "),s("td",[t._v("Own-child")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("20.0")]),t._v(" "),s("td",[t._v("63210.003417")]),t._v(" "),s("td",[t._v("9.0")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("6")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("Some-college")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("#na#")]),t._v(" "),s("td",[t._v("Other-relative")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("49.0")]),t._v(" "),s("td",[t._v("44434.000334")]),t._v(" "),s("td",[t._v("10.0")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("7")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("11th")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("#na#")]),t._v(" "),s("td",[t._v("Husband")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("37.0")]),t._v(" "),s("td",[t._v("138939.999209")]),t._v(" "),s("td",[t._v("7.0")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("8")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("HS-grad")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Craft-repair")]),t._v(" "),s("td",[t._v("Husband")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("False")]),t._v(" "),s("td",[t._v("46.0")]),t._v(" "),s("td",[t._v("328215.996478")]),t._v(" "),s("td",[t._v("9.0")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("9")]),t._v(" "),s("td",[t._v("Self-emp-inc")]),t._v(" "),s("td",[t._v("HS-grad")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("#na#")]),t._v(" "),s("td",[t._v("Husband")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("True")]),t._v(" "),s("td",[t._v("36.0")]),t._v(" "),s("td",[t._v("216711.000298")]),t._v(" "),s("td",[t._v("10.0")]),t._v(" "),s("td",[t._v(">=50k")])])])]),t._v(" "),s("p",[t._v("We have our labels! It'll grab them if possible by default!")]),t._v(" "),s("p",[t._v("What does that mean? Well, besides simply calling "),s("code",[t._v("get_preds")]),t._v(", we can also run "),s("code",[t._v("validate")]),t._v(" to see how a model performs. This is nice as it can allow for efficient methods when calculating something like permutation importance:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("validate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dl"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("dl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(#2) [0.4870152175426483,0.7699999809265137]\n")])])]),s("p",[t._v("We'll also show an example of "),s("code",[t._v("get_preds")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("preds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dl"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("dl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("tensor([0.5952, 0.4048])\n")])])]),s("p",[t._v("What would happen if I accidently passed in an unlablled dataset to "),s("code",[t._v("learn.validate")]),t._v(" though? Let's find out:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("df2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'salary'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("age")]),t._v(" "),s("th",[t._v("workclass")]),t._v(" "),s("th",[t._v("fnlwgt")]),t._v(" "),s("th",[t._v("education")]),t._v(" "),s("th",[t._v("education-num")]),t._v(" "),s("th",[t._v("marital-status")]),t._v(" "),s("th",[t._v("occupation")]),t._v(" "),s("th",[t._v("relationship")]),t._v(" "),s("th",[t._v("race")]),t._v(" "),s("th",[t._v("sex")]),t._v(" "),s("th",[t._v("capital-gain")]),t._v(" "),s("th",[t._v("capital-loss")]),t._v(" "),s("th",[t._v("hours-per-week")]),t._v(" "),s("th",[t._v("native-country")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("49")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("101320")]),t._v(" "),s("td",[t._v("Assoc-acdm")]),t._v(" "),s("td",[t._v("12.0")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("1902")]),t._v(" "),s("td",[t._v("40")]),t._v(" "),s("td",[t._v("United-States")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("44")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("236746")]),t._v(" "),s("td",[t._v("Masters")]),t._v(" "),s("td",[t._v("14.0")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("Exec-managerial")]),t._v(" "),s("td",[t._v("Not-in-family")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("Male")]),t._v(" "),s("td",[t._v("10520")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("45")]),t._v(" "),s("td",[t._v("United-States")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("38")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("96185")]),t._v(" "),s("td",[t._v("HS-grad")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Unmarried")]),t._v(" "),s("td",[t._v("Black")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("32")]),t._v(" "),s("td",[t._v("United-States")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("38")]),t._v(" "),s("td",[t._v("Self-emp-inc")]),t._v(" "),s("td",[t._v("112847")]),t._v(" "),s("td",[t._v("Prof-school")]),t._v(" "),s("td",[t._v("15.0")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Prof-specialty")]),t._v(" "),s("td",[t._v("Husband")]),t._v(" "),s("td",[t._v("Asian-Pac-Islander")]),t._v(" "),s("td",[t._v("Male")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("40")]),t._v(" "),s("td",[t._v("United-States")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("42")]),t._v(" "),s("td",[t._v("Self-emp-not-inc")]),t._v(" "),s("td",[t._v("82297")]),t._v(" "),s("td",[t._v("7th-8th")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Other-service")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("Black")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("50")]),t._v(" "),s("td",[t._v("United-States")])])])])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("dl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" learn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test_dl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("validate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dl"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("dl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("(#2) [None,None]\n")])])]),s("p",[t._v("We can see it will simply just return "),s("code",[t._v("None")]),t._v("!")])])}),[],!1,null,null,null);a.default=n.exports}}]);