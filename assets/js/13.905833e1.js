(window.webpackJsonp=window.webpackJsonp||[]).push([[13],{362:function(t,a,s){"use strict";s.r(a);var n=s(42),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"using-custom-transform-statistics-intermediate"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#using-custom-transform-statistics-intermediate"}},[t._v("#")]),t._v(" Using Custom Transform Statistics (Intermediate)")]),t._v(" "),s("blockquote",[s("p",[t._v("A guide for showing how to bring in pregenerated statistics for tabular")])]),t._v(" "),s("hr"),t._v(" "),s("p",[t._v("This article is also a Jupyter Notebook available to be run from the top down. There\nwill be code snippets that you can then run in any environment.")]),t._v(" "),s("p",[t._v("Below are the versions of "),s("code",[t._v("fastai")]),t._v(", "),s("code",[t._v("fastcore")]),t._v(", and "),s("code",[t._v("wwf")]),t._v(" currently running at the time of writing this:")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("fastai")]),t._v(": 2.0.16")]),t._v(" "),s("li",[s("code",[t._v("fastcore")]),t._v(": 1.1.2")]),t._v(" "),s("li",[s("code",[t._v("wwf")]),t._v(": 0.0.4")])]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"why-use-predetermined-stats"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#why-use-predetermined-stats"}},[t._v("#")]),t._v(" Why Use Predetermined Stats?")]),t._v(" "),s("p",[t._v("If "),s("code",[t._v("fastai")]),t._v(" will simply let us pass everything to a "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" object to preprocess and train on, why should having custom statistics for our data?")]),t._v(" "),s("p",[t._v("Let's try to think of a scenario.")]),t._v(" "),s("p",[t._v("My data is a few "),s("em",[t._v("trillion")]),t._v(" rows, so there is no way (currently) I can load this "),s("code",[t._v("DataFrame")]),t._v(" into memory at once. What do I do? Perhaps I would want to train on batches of my data at a time (article on this will come soon). To do this though I would "),s("strong",[t._v("need")]),t._v(" all of my "),s("code",[t._v("procs")]),t._v(" predetermined from the start so every transform is done the same across all of our mini-batches of data.")]),t._v(" "),s("p",[t._v("Currently there is an "),s("a",{attrs:{href:"https://github.com/fastai/fastai/pull/2837",target:"_blank",rel:"noopener noreferrer"}},[t._v("open PR"),s("OutboundLink")],1),t._v(" for this integration, so for now this will live inside of Walk with fastai and we'll show how to use it as well!")]),t._v(" "),s("p",[t._v("Before we begin, let's import the tabular module:")]),t._v(" "),s("h2",{attrs:{id:"modifying-the-procs"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#modifying-the-procs"}},[t._v("#")]),t._v(" Modifying the "),s("code",[t._v("procs")])]),t._v(" "),s("p",[t._v("Now let's modify each of our "),s("code",[t._v("procs")]),t._v(" to have this ability, as right now it's currently not there!")]),t._v(" "),s("h3",{attrs:{id:"categorify"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#categorify"}},[t._v("#")]),t._v(" Categorify")]),t._v(" "),s("p",[t._v("The first one we will look at is "),s("RouterLink",{attrs:{to:"/tab.stats.html#Categorify"}},[s("code",[t._v("Categorify")])]),t._v(". Currently the source code looks like so:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Categorify")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TabularProc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Transform the categorical variables to something similar to `pd.Categorical`"')]),t._v("\n    order "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setups")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        store_attr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("classes"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("CategoryMap"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" add_na"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" n "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("encodes")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" partial"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_apply_cats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("classes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("decodes")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" partial"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_decode_cats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("classes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__getitem__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("classes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("p",[t._v("What our modification needs to do is on the "),s("code",[t._v("__init__")]),t._v(" we need an option to pass in a dictionary of class mappings, and "),s("RouterLink",{attrs:{to:"/tab.stats.html#setups"}},[s("code",[t._v("setups")])]),t._v(" needs to generate class mappings for those not passed in. Let's do so below:")],1),t._v(" "),s("h2",{staticClass:"doc_header",attrs:{id:"Categorify"}},[s("code",[t._v("class")]),t._v(" "),s("code",[t._v("Categorify")]),s("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/tabular/core.py#L237"}},[t._v("[source]")])]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("Categorify")]),t._v("("),s("strong",[s("code",[t._v("enc")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("dec")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("split_idx")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("order")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(") :: "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularProc",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularProc")]),s("OutboundLink")],1)])]),t._v(" "),s("p",[t._v("Transform the categorical variables to something similar to "),s("code",[t._v("pd.Categorical")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Categorify")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TabularProc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Transform the categorical variables to something similar to `pd.Categorical`"')]),t._v("\n    order "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" classes"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" classes "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" classes "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" defaultdict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("L"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        store_attr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setups")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" n "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" n "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("classes "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("or")]),t._v(" is_categorical_dtype"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("classes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" CategoryMap"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" add_na"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("encodes")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" partial"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_apply_cats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("classes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("decodes")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" partial"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_decode_cats"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("classes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__getitem__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("classes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("p",[t._v("Now we have successfully set up our "),s("RouterLink",{attrs:{to:"/tab.stats.html#Categorify"}},[s("code",[t._v("Categorify")])]),t._v(". Let's look at a quick example below.")],1),t._v(" "),s("p",[t._v("We'll make a "),s("code",[t._v("DataFrame")]),t._v(" with two category columns:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("df "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'b'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'b'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'c'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'b'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("a")]),t._v(" "),s("th",[t._v("b")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("a")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v("b")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v("a")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("c")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v("b")])])])])]),t._v(" "),s("p",[t._v("Next we want to specify specific classes for "),s("code",[t._v("a")]),t._v(". We'll set a maximum range up to "),s("code",[t._v("4")]),t._v(" rather than "),s("code",[t._v("2")]),t._v(" shown in our "),s("code",[t._v("DataFrame")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("tst_classes "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("L"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'#na#'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" tst_classes\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("{'a': (#6) ['#na#',0,1,2,3,4]}\n")])])]),s("p",[t._v("Finally we will build a "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" object with a modified version of "),s("RouterLink",{attrs:{to:"/tab.stats.html#Categorify"}},[s("code",[t._v("Categorify")])]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularPandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Categorify"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("classes"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tst_classes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'b'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("How do we tell it worked though? Let's check out "),s("code",[t._v("to.classes")]),t._v(", which is a shortcut for "),s("code",[t._v("to.procs.categorify.classes")]),t._v(".")]),t._v(" "),s("p",[t._v("What we should see is our dictionary we mapped for "),s("code",[t._v("a")]),t._v(", which we do!")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("classes\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("{'a': (#6) ['#na#',0,1,2,3,4], 'b': ['#na#', 'a', 'b', 'c']}\n")])])]),s("h3",{attrs:{id:"normalize"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#normalize"}},[t._v("#")]),t._v(" Normalize")]),t._v(" "),s("p",[t._v("Next let's move onto "),s("a",{attrs:{href:"https://docs.fast.ai/data.transforms#Normalize",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Normalize")]),s("OutboundLink")],1),t._v(". Things get a bit tricky here because we also need to update the base "),s("a",{attrs:{href:"https://docs.fast.ai/data.transforms#Normalize",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Normalize")]),s("OutboundLink")],1),t._v(" transform as well.")]),t._v(" "),s("p",[t._v("Why?")]),t._v(" "),s("p",[t._v("Currently "),s("code",[t._v("fastai")]),t._v("'s "),s("a",{attrs:{href:"https://docs.fast.ai/data.transforms#Normalize",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Normalize")]),s("OutboundLink")],1),t._v(" tabular proc overrides the "),s("RouterLink",{attrs:{to:"/tab.stats.html#setups"}},[s("code",[t._v("setups")])]),t._v(" for "),s("a",{attrs:{href:"https://docs.fast.ai/data.transforms#Normalize",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Normalize")]),s("OutboundLink")],1),t._v(" by storing away our "),s("code",[t._v("means")]),t._v(" and "),s("code",[t._v("stds")]),t._v(". What we need to do is have an option to pass in our "),s("code",[t._v("means")]),t._v(" and "),s("code",[t._v("stds")]),t._v(" in the base "),s("a",{attrs:{href:"https://docs.fast.ai/data.transforms#Normalize",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Normalize")]),s("OutboundLink")],1),t._v(".")],1),t._v(" "),s("p",[t._v("Let's do so here with "),s("code",[t._v("@patch")])]),t._v(" "),s("h4",{staticClass:"doc_header",attrs:{id:"Normalize.__init__"}},[s("code",[t._v("Normalize.__init__")]),s("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/walkwithfastai/walkwithfastai.github.io/tree/master/wwf/tab/stats.py#L29"}},[t._v("[source]")])]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("Normalize."),s("strong",[t._v("init")])]),t._v("("),s("strong",[s("code",[t._v("x")])]),t._v(":"),s("a",{attrs:{href:"https://docs.fast.ai/data.transforms#Normalize",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Normalize")]),s("OutboundLink")],1),t._v(", "),s("strong",[s("code",[t._v("mean")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("std")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("axes")])]),t._v("="),s("em",[s("code",[t._v("(0, 2, 3)")])]),t._v(", "),s("strong",[s("code",[t._v("means")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(", "),s("strong",[s("code",[t._v("stds")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(")")])]),t._v(" "),s("p",[t._v("Initialize self.  See help(type(self)) for accurate signature.")]),t._v(" "),s("p",[t._v("Very nice little one-liner.")]),t._v(" "),s("p",[t._v("Integrating with tabular though will not be so nice as a one-liner. Our user scenario looks something like so:")]),t._v(" "),s("p",[t._v("We can pass in custom means "),s("em",[t._v("or")]),t._v(" custom standard deviations, and these should be in the form of a dictionary similar to how we had our "),s("code",[t._v("classes")]),t._v(" earlier. Let's modify "),s("RouterLink",{attrs:{to:"/tab.stats.html#setups"}},[s("code",[t._v("setups")])]),t._v(" to account for this:")],1),t._v(" "),s("h4",{staticClass:"doc_header",attrs:{id:"setups"}},[s("code",[t._v("setups")]),s("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/tabular/core.py#L371"}},[t._v("[source]")])]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("setups")]),t._v("("),s("strong",[s("code",[t._v("to")])]),t._v(":"),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#Tabular",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Tabular")]),s("OutboundLink")],1),t._v(")")])]),t._v(" "),s("p",[t._v("How do we test this?")]),t._v(" "),s("p",[t._v("We'll do a similar scenario to our "),s("RouterLink",{attrs:{to:"/tab.stats.html#Categorify"}},[s("code",[t._v("Categorify")])]),t._v(" example earlier. We'll have one column:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("df "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("a")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("0")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("1")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("2")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("3")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("4")])])])])]),t._v(" "),s("p",[t._v("And normalize them with some custom statistics. In our case we'll make them "),s("code",[t._v("3")]),t._v(" for a mean and "),s("code",[t._v("1")]),t._v(" for the standard deviation")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("tst_means"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("tst_stds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("We'll pass this into "),s("a",{attrs:{href:"https://docs.fast.ai/data.transforms#Normalize",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Normalize")]),s("OutboundLink")],1),t._v(" and build a "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" object:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("norm "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("means"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tst_means"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" stds"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tst_stds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nto "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularPandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" norm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cont_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We can then check our "),s("code",[t._v("mean")]),t._v(" and "),s("code",[t._v("std")]),t._v(" values:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("means"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stds\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("({'a': 3.0}, {'a': 1.0})\n")])])]),s("p",[t._v("And they line up!")]),t._v(" "),s("h3",{attrs:{id:"fillmissing"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#fillmissing"}},[t._v("#")]),t._v(" FillMissing")]),t._v(" "),s("p",[t._v("The last preprocesser is "),s("RouterLink",{attrs:{to:"/tab.stats.html#FillMissing"}},[s("code",[t._v("FillMissing")])]),t._v(". For this one we want to give "),s("code",[t._v("fastai")]),t._v(" the ability to accept custom "),s("code",[t._v("na_dicts")]),t._v(", as this is where the information is stored on what continuous columns contains missing values!")],1),t._v(" "),s("p",[t._v("Compared to the last two, this integration is pretty trivial. First we'll give "),s("code",[t._v("__init__")]),t._v(" the ability to accept a "),s("code",[t._v("na_dict")]),t._v(", then our "),s("RouterLink",{attrs:{to:"/tab.stats.html#setups"}},[s("code",[t._v("setups")])]),t._v(" needs to check if we have an "),s("code",[t._v("na_dict")]),t._v(" already and what columns aren't there from it. First let's look at the old:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FillMissing")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TabularProc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Fill the missing values in continuous columns."')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fill_strategy"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("FillStrategy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("median"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" add_col"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fill_vals"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" fill_vals "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" fill_vals "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" defaultdict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        store_attr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setups")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        missing "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isnull"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("any")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        store_attr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("na_dict"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fill_strategy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dsets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fill_vals"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" n "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" missing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("missing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keys"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fill_strategy "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fill_strategy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__name__\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("encodes")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        missing "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isnull"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" n "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" missing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("any")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("missing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("any")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keys"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" n "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("na_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"nan values in `')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('` but not in setup training set"')])]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" n "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("na_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keys"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fillna"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("na_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inplace"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_col"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_na'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" missing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_na'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_na'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Followed by our new:")]),t._v(" "),s("h2",{staticClass:"doc_header",attrs:{id:"FillMissing"}},[s("code",[t._v("class")]),t._v(" "),s("code",[t._v("FillMissing")]),s("a",{staticClass:"source_link",staticStyle:{float:"right"},attrs:{href:"https://github.com/fastai/fastai/tree/master/fastai/tabular/core.py#L293"}},[t._v("[source]")])]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("FillMissing")]),t._v("("),s("strong",[s("code",[t._v("fill_strategy")])]),t._v("="),s("em",[s("code",[t._v("median")])]),t._v(", "),s("strong",[s("code",[t._v("add_col")])]),t._v("="),s("em",[s("code",[t._v("True")])]),t._v(", "),s("strong",[s("code",[t._v("fill_vals")])]),t._v("="),s("em",[s("code",[t._v("None")])]),t._v(") :: "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularProc",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularProc")]),s("OutboundLink")],1)])]),t._v(" "),s("p",[t._v("Fill the missing values in continuous columns.")]),t._v(" "),s("p",[t._v("We can see our "),s("RouterLink",{attrs:{to:"/tab.stats.html#setups"}},[s("code",[t._v("setups")])]),t._v(" checks for what new "),s("code",[t._v("cont_names")]),t._v(" we have and then updates our "),s("code",[t._v("na_dict")]),t._v(" with those missing keys. Let's test it out below:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("df "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nan"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'b'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nan"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("a")]),t._v(" "),s("th",[t._v("b")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("0.0")]),t._v(" "),s("td",[t._v("NaN")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("1.0")]),t._v(" "),s("td",[t._v("1.0")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("2.0")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("1.0")]),t._v(" "),s("td",[t._v("3.0")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("2.0")]),t._v(" "),s("td",[t._v("4.0")])])])])]),t._v(" "),s("p",[t._v("We'll pass in a dictionary for "),s("code",[t._v("a")]),t._v(" but not "),s("code",[t._v("b")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("fill "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" FillMissing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("na_dict"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \nto "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularPandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fill"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cont_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'b'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("And now let's look at our "),s("code",[t._v("na_dict")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("na_dict\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("{'a': 2.0, 'b': 3.5}\n")])])]),s("p",[t._v("We can see that it all works!")]),t._v(" "),s("h2",{attrs:{id:"full-integration-example"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#full-integration-example"}},[t._v("#")]),t._v(" Full Integration Example")]),t._v(" "),s("p",[t._v("Nor for those folks that don't particularly care about how we get to this point and simply want to use it, we'll do the following:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" wwf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tab"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stats "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fastai"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tabular"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("all")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n")])])]),s("p",[t._v("We'll make an example from the "),s("code",[t._v("ADULT_SAMPLE")]),t._v(" dataset:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("path "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" untar_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("URLs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ADULT_SAMPLE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'adult.csv'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("age")]),t._v(" "),s("th",[t._v("workclass")]),t._v(" "),s("th",[t._v("fnlwgt")]),t._v(" "),s("th",[t._v("education")]),t._v(" "),s("th",[t._v("education-num")]),t._v(" "),s("th",[t._v("marital-status")]),t._v(" "),s("th",[t._v("occupation")]),t._v(" "),s("th",[t._v("relationship")]),t._v(" "),s("th",[t._v("race")]),t._v(" "),s("th",[t._v("sex")]),t._v(" "),s("th",[t._v("capital-gain")]),t._v(" "),s("th",[t._v("capital-loss")]),t._v(" "),s("th",[t._v("hours-per-week")]),t._v(" "),s("th",[t._v("native-country")]),t._v(" "),s("th",[t._v("salary")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("49")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("101320")]),t._v(" "),s("td",[t._v("Assoc-acdm")]),t._v(" "),s("td",[t._v("12.0")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("1902")]),t._v(" "),s("td",[t._v("40")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("44")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("236746")]),t._v(" "),s("td",[t._v("Masters")]),t._v(" "),s("td",[t._v("14.0")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("Exec-managerial")]),t._v(" "),s("td",[t._v("Not-in-family")]),t._v(" "),s("td",[t._v("White")]),t._v(" "),s("td",[t._v("Male")]),t._v(" "),s("td",[t._v("10520")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("45")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("38")]),t._v(" "),s("td",[t._v("Private")]),t._v(" "),s("td",[t._v("96185")]),t._v(" "),s("td",[t._v("HS-grad")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Divorced")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Unmarried")]),t._v(" "),s("td",[t._v("Black")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("32")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v("<50k")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("38")]),t._v(" "),s("td",[t._v("Self-emp-inc")]),t._v(" "),s("td",[t._v("112847")]),t._v(" "),s("td",[t._v("Prof-school")]),t._v(" "),s("td",[t._v("15.0")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Prof-specialty")]),t._v(" "),s("td",[t._v("Husband")]),t._v(" "),s("td",[t._v("Asian-Pac-Islander")]),t._v(" "),s("td",[t._v("Male")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("40")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v(">=50k")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("42")]),t._v(" "),s("td",[t._v("Self-emp-not-inc")]),t._v(" "),s("td",[t._v("82297")]),t._v(" "),s("td",[t._v("7th-8th")]),t._v(" "),s("td",[t._v("NaN")]),t._v(" "),s("td",[t._v("Married-civ-spouse")]),t._v(" "),s("td",[t._v("Other-service")]),t._v(" "),s("td",[t._v("Wife")]),t._v(" "),s("td",[t._v("Black")]),t._v(" "),s("td",[t._v("Female")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v("50")]),t._v(" "),s("td",[t._v("United-States")]),t._v(" "),s("td",[t._v("<50k")])])])])]),t._v(" "),s("p",[t._v("We'll set everything up as we normally would for our "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("cat_names "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'workclass'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'marital-status'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'occupation'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relationship'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'race'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ncont_names "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'age'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fnlwgt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education-num'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nsplits "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" RandomSplitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("range_of"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Except we'll define every proc ourselves. For our "),s("RouterLink",{attrs:{to:"/tab.stats.html#Categorify"}},[s("code",[t._v("Categorify")])]),t._v(" example we will use "),s("code",[t._v("relationship")]),t._v(", "),s("a",{attrs:{href:"https://docs.fast.ai/data.transforms#Normalize",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Normalize")]),s("OutboundLink")],1),t._v(" will use "),s("code",[t._v("age")]),t._v(", and "),s("RouterLink",{attrs:{to:"/tab.stats.html#FillMissing"}},[s("code",[t._v("FillMissing")])]),t._v(" will use "),s("code",[t._v("education-num")]),t._v(":")],1),t._v(" "),s("h3",{attrs:{id:"categorify-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#categorify-2"}},[t._v("#")]),t._v(" Categorify")]),t._v(" "),s("p",[t._v("First let's find those values:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relationship'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unique"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("array([' Wife', ' Not-in-family', ' Unmarried', ' Husband', ' Own-child',\n       ' Other-relative'], dtype=object)\n")])])]),s("p",[t._v("And we'll set that as a dictionary with a "),s("code",[t._v("Single")]),t._v(" class as well:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("classes "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relationship'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relationship'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unique"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("' Single '")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("And pass it to our "),s("RouterLink",{attrs:{to:"/tab.stats.html#Categorify"}},[s("code",[t._v("Categorify")])]),t._v(":")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("cat "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Categorify"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("classes"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("classes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"normalize-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#normalize-2"}},[t._v("#")]),t._v(" Normalize")]),t._v(" "),s("p",[t._v("Next we have normalize. We'll use a (very) wrong mean and standard deviation of 15. and 7.:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("means"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("stds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'age'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'age'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("And pass it to "),s("a",{attrs:{href:"https://docs.fast.ai/data.transforms#Normalize",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Normalize")]),s("OutboundLink")],1),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("norm "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Normalize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("means"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("means"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" stds"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("stds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"fillmissing-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#fillmissing-2"}},[t._v("#")]),t._v(" FillMissing")]),t._v(" "),s("p",[t._v("Lastly we have our "),s("RouterLink",{attrs:{to:"/tab.stats.html#FillMissing"}},[s("code",[t._v("FillMissing")])]),t._v(", which we will simply fill with 5.:")],1),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("na_dict "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'education-num'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("And pass it in:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("fill "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" FillMissing"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("na_dict"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("na_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n")])])]),s("h3",{attrs:{id:"bringing-it-together"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#bringing-it-together"}},[t._v("#")]),t._v(" Bringing it together")]),t._v(" "),s("p",[t._v("Now let's build our "),s("a",{attrs:{href:"https://docs.fast.ai/tabular.core#TabularPandas",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("TabularPandas")]),s("OutboundLink")],1),t._v(" object:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("procs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("cat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" norm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fill"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nto "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TabularPandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" procs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("procs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cat_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cat_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cont_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cont_names"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   y_names"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'salary'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" splits"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("splits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("{% include note.html content='you may need to redefine your "),s("code",[t._v("cat_names")]),t._v(" and "),s("code",[t._v("cont_names")]),t._v(" here, this is because "),s("RouterLink",{attrs:{to:"/tab.stats.html#FillMissing"}},[s("code",[t._v("FillMissing")])]),t._v(" may override them' %}")],1),t._v(" "),s("p",[t._v("And we're done!")]),t._v(" "),s("p",[t._v("Thanks again for reading, and I hope this article helps you with your tabular endeavors!")])])}),[],!1,null,null,null);a.default=e.exports}}]);